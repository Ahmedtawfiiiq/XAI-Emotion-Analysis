{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/yidinghao/interpreting-nlp#egg=interpret_nlp\n!pip install yattag","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AnqcD9M0EIG8","outputId":"cfa6ed73-66de-407a-9dbe-6eacd214899a","execution":{"iopub.status.busy":"2023-08-22T17:02:32.452834Z","iopub.execute_input":"2023-08-22T17:02:32.453098Z","iopub.status.idle":"2023-08-22T17:03:02.514724Z","shell.execute_reply.started":"2023-08-22T17:02:32.453072Z","shell.execute_reply":"2023-08-22T17:03:02.513524Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting interpret_nlp\n  Cloning https://github.com/yidinghao/interpreting-nlp to /tmp/pip-install-j26tgc9l/interpret-nlp_88fb8e748f314a8fb1e22079b2d6a13c\n  Running command git clone --filter=blob:none --quiet https://github.com/yidinghao/interpreting-nlp /tmp/pip-install-j26tgc9l/interpret-nlp_88fb8e748f314a8fb1e22079b2d6a13c\n  Resolved https://github.com/yidinghao/interpreting-nlp to commit b6bf81e2e52970ffe4db32cade8c9c3d48d1cd3b\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: interpret_nlp\n  Building wheel for interpret_nlp (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for interpret_nlp: filename=interpret_nlp-0.1-py3-none-any.whl size=14714 sha256=9f2fc79cef1d9fa37d5379bc3f897b0934eec10c0370c355a2813fca943488a4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-juhd5gb3/wheels/c0/d4/0f/9d21571e687d4e3dcf61b6538dedff103fdc48bad18ca6c357\nSuccessfully built interpret_nlp\nInstalling collected packages: interpret_nlp\nSuccessfully installed interpret_nlp-0.1\nCollecting yattag\n  Downloading yattag-1.15.1.tar.gz (28 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: yattag\n  Building wheel for yattag (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for yattag: filename=yattag-1.15.1-py3-none-any.whl size=15649 sha256=d8a37396155ff505e0cfc971a10a21d868ea57a83ddf5ce1515830e00044e796\n  Stored in directory: /root/.cache/pip/wheels/c0/f7/67/89165fb6c0e73ad142c1a60daed9e773af08961396d06fa391\nSuccessfully built yattag\nInstalling collected packages: yattag\nSuccessfully installed yattag-1.15.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gdown\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:03:02.517131Z","iopub.execute_input":"2023-08-22T17:03:02.517856Z","iopub.status.idle":"2023-08-22T17:03:13.871584Z","shell.execute_reply.started":"2023-08-22T17:03:02.517813Z","shell.execute_reply":"2023-08-22T17:03:13.870422Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom interpret_nlp.modules.lrp_modules import LRPLinear, LRPGRU","metadata":{"id":"c84CkUNzLgLO","execution":{"iopub.status.busy":"2023-08-22T17:03:13.873336Z","iopub.execute_input":"2023-08-22T17:03:13.873949Z","iopub.status.idle":"2023-08-22T17:03:24.814415Z","shell.execute_reply.started":"2023-08-22T17:03:13.873908Z","shell.execute_reply":"2023-08-22T17:03:24.813321Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id  1-5vw77DSI1gsMdnquVXkZrA9xai57f4o\n!gdown --id  1-4ss3i6KEvWWuFegO95aB3itKopXwM3T\n!gdown --id  1ItgcQIno9rkbyIuDv3xtU6PRjURBIgos\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:07:44.825014Z","iopub.execute_input":"2023-08-22T17:07:44.825790Z","iopub.status.idle":"2023-08-22T17:08:19.250994Z","shell.execute_reply.started":"2023-08-22T17:07:44.825756Z","shell.execute_reply":"2023-08-22T17:08:19.249775Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1-5vw77DSI1gsMdnquVXkZrA9xai57f4o\nFrom (redirected): https://drive.google.com/uc?id=1-5vw77DSI1gsMdnquVXkZrA9xai57f4o&confirm=t&uuid=66a2843b-1bae-4eb5-beac-482a504fdeeb\nTo: /kaggle/working/ls_val_embeddings.pkl\n100%|████████████████████████████████████████| 464M/464M [00:06<00:00, 73.4MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1-4ss3i6KEvWWuFegO95aB3itKopXwM3T\nFrom (redirected): https://drive.google.com/uc?id=1-4ss3i6KEvWWuFegO95aB3itKopXwM3T&confirm=t&uuid=b12788e7-5280-497c-9c30-58cbeacd1e95\nTo: /kaggle/working/ls_train_embeddings.pkl\n100%|███████████████████████████████████████| 2.16G/2.16G [00:16<00:00, 134MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1ItgcQIno9rkbyIuDv3xtU6PRjURBIgos\nFrom (redirected): https://drive.google.com/uc?id=1ItgcQIno9rkbyIuDv3xtU6PRjURBIgos&confirm=t&uuid=d3a54677-39be-481a-b04f-984b4b6327c7\nTo: /kaggle/working/ls_test_embeddings.pkl\n100%|████████████████████████████████████████| 464M/464M [00:06<00:00, 71.7MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1-8HVDyZkJwLxn4Dc-SqNWlZZkd3jFvF_\n!gdown --id 1-7saiHjQojSkvxbrkz1d4ebryH252Iox\n!gdown --id 1-D-vwSDK6qPH2TN8owr1Uy3JQSKitpVH","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:08:33.527953Z","iopub.execute_input":"2023-08-22T17:08:33.528546Z","iopub.status.idle":"2023-08-22T17:08:39.371571Z","shell.execute_reply.started":"2023-08-22T17:08:33.528505Z","shell.execute_reply":"2023-08-22T17:08:39.370426Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1-8HVDyZkJwLxn4Dc-SqNWlZZkd3jFvF_\nTo: /kaggle/working/train.pkl\n100%|███████████████████████████████████████| 3.00M/3.00M [00:00<00:00, 176MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1-7saiHjQojSkvxbrkz1d4ebryH252Iox\nTo: /kaggle/working/val.pkl\n100%|█████████████████████████████████████████| 643k/643k [00:00<00:00, 118MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1-D-vwSDK6qPH2TN8owr1Uy3JQSKitpVH\nTo: /kaggle/working/test.pkl\n100%|█████████████████████████████████████████| 638k/638k [00:00<00:00, 101MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/working/train.pkl', 'rb') as f:\n    train = pickle.load(f)\n    \nwith open('/kaggle/working/val.pkl', 'rb') as f:\n    val = pickle.load(f)\n\nwith open('/kaggle/working/test.pkl', 'rb') as f:\n    test = pickle.load(f)\n\nwith open('/kaggle/working/ls_train_embeddings.pkl', 'rb') as f:\n    train_embeddings = pickle.load(f)\n    \nwith open('/kaggle/working/ls_val_embeddings.pkl', 'rb') as f:\n    val_embeddings = pickle.load(f)\n    \nwith open('/kaggle/working/ls_test_embeddings.pkl', 'rb') as f:\n    test_embeddings = pickle.load(f)","metadata":{"id":"SFQdDhAzQXdr","execution":{"iopub.status.busy":"2023-08-22T17:08:46.657792Z","iopub.execute_input":"2023-08-22T17:08:46.658195Z","iopub.status.idle":"2023-08-22T17:08:48.408892Z","shell.execute_reply.started":"2023-08-22T17:08:46.658159Z","shell.execute_reply":"2023-08-22T17:08:48.407466Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"LRP Implementaion","metadata":{"id":"2qL9ODCUfJCC"}},{"cell_type":"code","source":"train.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"9qeoqzwiRm4r","outputId":"9e9087a9-ec6c-4bde-d27f-71ae001c1ab4","execution":{"iopub.status.busy":"2023-08-22T17:08:53.122905Z","iopub.execute_input":"2023-08-22T17:08:53.123279Z","iopub.status.idle":"2023-08-22T17:08:53.153060Z","shell.execute_reply.started":"2023-08-22T17:08:53.123250Z","shell.execute_reply":"2023-08-22T17:08:53.152045Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                  tweet  label  \\\n5081  1996   يا مهند ياحبيبي انت تدور لبش مع اي احد ...      1   \n8264  حلب عفرين  اعتقال عشرات الشباب من قريه باسوطه ...      3   \n9777             انا نفسي مره اجرب اكون ليك حد اقرب : )      3   \n740   الصراحه احساس مخزي ونحن نكتفي بالتمثيل المشرف ...      3   \n6537  57   عاجل الايطالي فابيو باسل يفوز بدهب الاولي...      0   \n\n                             tweet_with_replaced_emojis  \\\n5081  1996   يا مهند ياحبيبي انت تدور لبش مع اي احد ...   \n8264  حلب عفرين  اعتقال عشرات الشباب من قريه باسوطه ...   \n9777             انا نفسي مره اجرب اكون ليك حد اقرب : )   \n740   الصراحه احساس مخزي ونحن نكتفي بالتمثيل المشرف ...   \n6537  57   عاجل الايطالي فابيو باسل يفوز بدهب الاولي...   \n\n                      cleaned_data_with_replaced_emojis  \\\n5081  يامهند ياحبيبي تدور لبش اي احد متاكد ابو جاسم ...   \n8264  حلب عفرين اعتقال عشرات الشباب قريه باسوطه ريف ...   \n9777                             مره اجرب اكون ليك اقرب   \n740   الصراحه احساس مخزي نكتفي بالتمثيل المشرف الاول...   \n6537  عاجل الايطالي فابيو باسل يفوز بدهب الاوليمبياد...   \n\n                                         Light Stemming  \n5081  يامهند ياحبيبي تدور لبش اي احد متاكد ابو جاسم ...  \n8264  حلب عفر اعتقال عشر الشباب قريه باسوطه ريف عفر ...  \n9777                             مره اجرب اكون ليك اقرب  \n740   الصراحه احساس مخزي نكتفي بالتمثيل المشرف الاول...  \n6537  عاجل الايطالي فابيو باسل يفوز بدهب الاوليمبياد...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>label</th>\n      <th>tweet_with_replaced_emojis</th>\n      <th>cleaned_data_with_replaced_emojis</th>\n      <th>Light Stemming</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5081</th>\n      <td>1996   يا مهند ياحبيبي انت تدور لبش مع اي احد ...</td>\n      <td>1</td>\n      <td>1996   يا مهند ياحبيبي انت تدور لبش مع اي احد ...</td>\n      <td>يامهند ياحبيبي تدور لبش اي احد متاكد ابو جاسم ...</td>\n      <td>يامهند ياحبيبي تدور لبش اي احد متاكد ابو جاسم ...</td>\n    </tr>\n    <tr>\n      <th>8264</th>\n      <td>حلب عفرين  اعتقال عشرات الشباب من قريه باسوطه ...</td>\n      <td>3</td>\n      <td>حلب عفرين  اعتقال عشرات الشباب من قريه باسوطه ...</td>\n      <td>حلب عفرين اعتقال عشرات الشباب قريه باسوطه ريف ...</td>\n      <td>حلب عفر اعتقال عشر الشباب قريه باسوطه ريف عفر ...</td>\n    </tr>\n    <tr>\n      <th>9777</th>\n      <td>انا نفسي مره اجرب اكون ليك حد اقرب : )</td>\n      <td>3</td>\n      <td>انا نفسي مره اجرب اكون ليك حد اقرب : )</td>\n      <td>مره اجرب اكون ليك اقرب</td>\n      <td>مره اجرب اكون ليك اقرب</td>\n    </tr>\n    <tr>\n      <th>740</th>\n      <td>الصراحه احساس مخزي ونحن نكتفي بالتمثيل المشرف ...</td>\n      <td>3</td>\n      <td>الصراحه احساس مخزي ونحن نكتفي بالتمثيل المشرف ...</td>\n      <td>الصراحه احساس مخزي نكتفي بالتمثيل المشرف الاول...</td>\n      <td>الصراحه احساس مخزي نكتفي بالتمثيل المشرف الاول...</td>\n    </tr>\n    <tr>\n      <th>6537</th>\n      <td>57   عاجل الايطالي فابيو باسل يفوز بدهب الاولي...</td>\n      <td>0</td>\n      <td>57   عاجل الايطالي فابيو باسل يفوز بدهب الاولي...</td>\n      <td>عاجل الايطالي فابيو باسل يفوز بدهب الاوليمبياد...</td>\n      <td>عاجل الايطالي فابيو باسل يفوز بدهب الاوليمبياد...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_embeddings.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIJwvMFCRvLD","outputId":"1b269fee-c7d7-4bed-d320-3fa6532248d1","execution":{"iopub.status.busy":"2023-08-22T17:09:04.218008Z","iopub.execute_input":"2023-08-22T17:09:04.218398Z","iopub.status.idle":"2023-08-22T17:09:04.224684Z","shell.execute_reply.started":"2023-08-22T17:09:04.218367Z","shell.execute_reply":"2023-08-22T17:09:04.223720Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(7045, 50, 768)"},"metadata":{}}]},{"cell_type":"code","source":"X_train = train['Light Stemming']\ny_train = train['label']\nX_val = val['Light Stemming']\ny_val = val['label']\nX_test = test['Light Stemming']\ny_test = test['label']","metadata":{"id":"RL9a8X96UjEo","execution":{"iopub.status.busy":"2023-08-22T17:09:06.578908Z","iopub.execute_input":"2023-08-22T17:09:06.579623Z","iopub.status.idle":"2023-08-22T17:09:06.585538Z","shell.execute_reply.started":"2023-08-22T17:09:06.579584Z","shell.execute_reply":"2023-08-22T17:09:06.584382Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"encoder = OneHotEncoder()\n\ny_train = encoder.fit_transform(train['label'].values.reshape(-1,1)).toarray()\ny_val = encoder.transform(val['label'].values.reshape(-1,1)).toarray()\ny_test = encoder.transform(test['label'].values.reshape(-1,1)).toarray()","metadata":{"id":"JG5WQoQaTpSh","execution":{"iopub.status.busy":"2023-08-22T14:11:17.994975Z","iopub.execute_input":"2023-08-22T14:11:17.995297Z","iopub.status.idle":"2023-08-22T14:11:18.027040Z","shell.execute_reply.started":"2023-08-22T14:11:17.995257Z","shell.execute_reply":"2023-08-22T14:11:18.025943Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass GRUModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, max_length, dropout_prob):\n        super(GRUModel, self).__init__()\n\n        self.gru1 = LRPGRU(input_size, hidden_size)\n        self.dropout1 = nn.Dropout(dropout_prob)\n        \n        self.gru2 = LRPGRU(hidden_size, hidden_size)\n        self.dropout2 = nn.Dropout(dropout_prob)\n        \n        self.gru3 = LRPGRU(hidden_size, hidden_size)\n        self.dropout3 = nn.Dropout(dropout_prob)\n        \n        self.fc = LRPLinear(hidden_size, output_size)  # Use LRPLinear\n        self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self, x):\n        gru_out1, _ = self.gru1(x)\n        gru_out1 = self.dropout1(gru_out1)\n        \n        gru_out2, _ = self.gru2(gru_out1)\n        gru_out2 = self.dropout2(gru_out2)\n        \n        gru_out3, _ = self.gru3(gru_out2)\n        gru_out3 = self.dropout3(gru_out3)\n        \n        output = self.fc(gru_out3)\n        output = self.softmax(output)\n        \n        return output\n    \n   \n","metadata":{"id":"t6kifeMSNnGF","execution":{"iopub.status.busy":"2023-08-22T14:11:18.030764Z","iopub.execute_input":"2023-08-22T14:11:18.031113Z","iopub.status.idle":"2023-08-22T14:11:18.042037Z","shell.execute_reply.started":"2023-08-22T14:11:18.031063Z","shell.execute_reply":"2023-08-22T14:11:18.040583Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define your constants\nINPUT_SIZE = 768  # Since each word is represented by a 768-dimensional embedding\nHIDDEN_SIZE = 64\nOUTPUT_SIZE = 8\nMAX_LENGTH = 50\nDROPOUT_PROB = 0.2\nLEARNING_RATE =  0.0053516485623658835\n\n# Instantiate the GRUModel\nmodel = GRUModel(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE,\n                 output_size=OUTPUT_SIZE, max_length=MAX_LENGTH, dropout_prob=DROPOUT_PROB)\n\n# Define your optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Define your loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Convert input data to PyTorch tensors (train_embeddings, val_embeddings, test_embeddings)\ntrain_data = torch.tensor(train_embeddings, dtype=torch.float32)\nval_data = torch.tensor(val_embeddings, dtype=torch.float32)\ntest_data = torch.tensor(test_embeddings, dtype=torch.float32)\n\n# Convert labels to PyTorch tensors (train_labels, val_labels, test_labels)\ntrain_labels = torch.tensor(y_train, dtype=torch.long)\nval_labels = torch.tensor(y_val, dtype=torch.long)\ntest_labels = torch.tensor(y_test, dtype=torch.long)\nNUM_EPOCHS = 10\n# Training loop\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    optimizer.zero_grad()\n    train_outputs = model(train_data)\n    loss = criterion(train_outputs, train_labels)\n    loss.backward()\n    optimizer.step()\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_outputs = model(val_data)\n        val_loss = criterion(val_outputs, val_labels)\n    \n    print(f\"Epoch {epoch+1}: Train Loss={loss.item():.4f}, Val Loss={val_loss.item():.4f}\")\n\n# Testing\nmodel.eval()\nwith torch.no_grad():\n    test_outputs = model(test_data)\n    test_predicted = torch.argmax(test_outputs, dim=1)\n    \n# Calculate accuracy on test set\ncorrect = (test_predicted == test_labels).sum().item()\ntotal = len(test_labels)\naccuracy = correct / total\nprint(f\"Test Accuracy: {100*accuracy:.2f}\")\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-08-22T15:52:35.336783Z","iopub.execute_input":"2023-08-22T15:52:35.338026Z","iopub.status.idle":"2023-08-22T15:53:40.140241Z","shell.execute_reply.started":"2023-08-22T15:52:35.337967Z","shell.execute_reply":"2023-08-22T15:53:40.138911Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss=3.9116, Val Loss=3.9026\nEpoch 2: Train Loss=3.9033, Val Loss=3.8882\nEpoch 3: Train Loss=3.8900, Val Loss=3.8642\nEpoch 4: Train Loss=3.8675, Val Loss=3.8304\nEpoch 5: Train Loss=3.8350, Val Loss=3.7868\nEpoch 6: Train Loss=3.7927, Val Loss=3.7339\nEpoch 7: Train Loss=3.7408, Val Loss=3.6683\nEpoch 8: Train Loss=3.6770, Val Loss=3.5838\nEpoch 9: Train Loss=3.5943, Val Loss=3.4863\nEpoch 10: Train Loss=3.4987, Val Loss=3.3926\nTest Accuracy: 699.54\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model state dictionary\ntorch.save(model.state_dict(), 'model_state.pth')","metadata":{"id":"nRPz4JnR6tTm","execution":{"iopub.status.busy":"2023-08-22T14:13:14.551898Z","iopub.execute_input":"2023-08-22T14:13:14.552875Z","iopub.status.idle":"2023-08-22T14:13:14.562542Z","shell.execute_reply.started":"2023-08-22T14:13:14.552841Z","shell.execute_reply":"2023-08-22T14:13:14.561319Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\nmodel.load_state_dict(torch.load('model_state.pth'))\nmodel.eval()  # Set the model to evaluation mode","metadata":{"id":"y2LTTswQ9Fe2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af56a1a5-7fa4-4848-af21-f05da4649420","execution":{"iopub.status.busy":"2023-08-22T14:13:14.564016Z","iopub.execute_input":"2023-08-22T14:13:14.564498Z","iopub.status.idle":"2023-08-22T14:13:14.582975Z","shell.execute_reply.started":"2023-08-22T14:13:14.564456Z","shell.execute_reply":"2023-08-22T14:13:14.581898Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"GRUModel(\n  (gru1): LRPGRU(768, 64, batch_first=True)\n  (dropout1): Dropout(p=0.2, inplace=False)\n  (gru2): LRPGRU(64, 64, batch_first=True)\n  (dropout2): Dropout(p=0.2, inplace=False)\n  (gru3): LRPGRU(64, 64, batch_first=True)\n  (dropout3): Dropout(p=0.2, inplace=False)\n  (fc): LRPLinear(in_features=64, out_features=8, bias=True)\n  (softmax): Softmax(dim=1)\n)"},"metadata":{}}]},{"cell_type":"code","source":"test_data[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-08-22T15:38:38.217623Z","iopub.execute_input":"2023-08-22T15:38:38.219051Z","iopub.status.idle":"2023-08-22T15:38:38.226171Z","shell.execute_reply.started":"2023-08-22T15:38:38.218996Z","shell.execute_reply":"2023-08-22T15:38:38.225334Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"torch.Size([50, 768])"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef explain_lrp(model, x):\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        # LRP forward\n        model.fc.attr()\n        model.gru1.attr()\n        model.gru2.attr()\n        model.gru3.attr()\n\n        gru_out1 = model.gru1(x)\n        print(\"gru_out1 shape:\", gru_out1.shape)\n\n        gru_out2 = model.gru2(gru_out1)\n        print(\"gru_out2 shape:\", gru_out2.shape)\n\n        gru_out3= model.gru3(gru_out2)\n        print(\"gru_out3 shape:\", gru_out3.shape)\n        \n        gru_out = gru_out3\n        h_final = np.concatenate((gru_out[:, -1, :model.gru3.hidden_size],\n                                  gru_out[:, 0, model.gru3.hidden_size:]), axis=-1)\n        h_final_reshaped = h_final.reshape(-1, model.fc.weight.shape[1])\n        output = model.fc(h_final_reshaped)\n\n        # LRP backward\n        target_class = np.argmax(output, axis=1)\n        rel_output = np.zeros_like(output)\n        rel_output[np.arange(output.shape[0]), target_class] = output[np.arange(output.shape[0]), target_class]\n        rel_h_final = model.fc.attr_backward(rel_output)\n        print(\"rel_h_final shape:\", rel_h_final.shape)\n\n        rel_gru_out = np.zeros(gru_out.shape)\n        rel_gru_out[:, -1, :model.gru3.hidden_size] = rel_h_final[:, :model.gru3.hidden_size]\n        rel_gru_out[:, 0, model.gru3.hidden_size:] = rel_h_final[:, model.gru3.hidden_size:]\n        print(\"rel_gru_out shape:\", rel_gru_out.shape)\n\n        rel_embeddings = model.gru1.attr_backward(rel_gru_out)\n        print(\"rel_embeddings shape:\", rel_embeddings.shape)\n\n        return rel_embeddings\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T15:09:33.321454Z","iopub.execute_input":"2023-08-22T15:09:33.321879Z","iopub.status.idle":"2023-08-22T15:09:33.334898Z","shell.execute_reply.started":"2023-08-22T15:09:33.321847Z","shell.execute_reply":"2023-08-22T15:09:33.333983Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_relevance_scores(rel_scores):\n    plt.figure(figsize=(10, 6))\n    plt.imshow(rel_scores, cmap='viridis', aspect='auto')  # Use a suitable colormap\n    plt.colorbar(label='Relevance Score')\n    plt.xlabel('Embedding Dimension')\n    plt.ylabel('Time Step')\n    plt.title('Heatmap of Relevance Scores')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T15:09:36.404172Z","iopub.execute_input":"2023-08-22T15:09:36.404602Z","iopub.status.idle":"2023-08-22T15:09:36.411498Z","shell.execute_reply.started":"2023-08-22T15:09:36.404571Z","shell.execute_reply":"2023-08-22T15:09:36.409957Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# another model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\nclass GRUModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n        super(GRUModel, self).__init__()\n\n        self.gru =LRPGRU(input_size, hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.dense = LRPLinear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        _, h_n = self.gru(x)\n        out = self.dropout(h_n[-1, :, :])  # Using the last hidden state\n        out = self.dense(out)\n        out = self.softmax(out)\n        return out\n\n# Parameters\nEPOCHS = 50\nLEARNING_RATE = 0.0053516485623658835\nINPUT_SIZE = 768\nHIDDEN_SIZE = 256\nOUTPUT_SIZE = 8  # Output size for softmax layer\nDROPOUT_RATE = 0.011691195516528197\npatience = 10\nBATCH_SIZE = 64\n\n\n# Create the model\nmodel = GRUModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, DROPOUT_RATE)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()  # Cross-Entropy loss for classification\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Display the model architecture\nprint(model)\n\n     \n\n\n#preparing the labels for traning\nfrom tensorflow.keras.utils import to_categorical\ny_train_one_hot = to_categorical(train['label'], num_classes=8)\ny_val_one_hot   = to_categorical(val['label'], num_classes=8)\ny_test_one_hot  = to_categorical(test['label'], num_classes=8)\n     \n\n\n   \n\n\n# Convert NumPy arrays to PyTorch tensors\ntrain_embedd_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\ny_train_one_hot_tensor = torch.tensor(y_train_one_hot, dtype=torch.float32)\n\nval_embedd_tensor = torch.tensor(val_embeddings, dtype=torch.float32)\ny_val_one_hot_tensor = torch.tensor(y_val_one_hot, dtype=torch.float32)\n\n# Initialize the model, loss function, and optimizer\nmodel = GRUModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, DROPOUT_RATE)\ncriterion = nn.CrossEntropyLoss()  # Assuming labels are integers, not one-hot encoded\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n     \n\n\n\n     \n\n\n# Create data loaders\ntrain_dataset = torch.utils.data.TensorDataset(train_embedd_tensor, y_train_one_hot_tensor)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nval_dataset = torch.utils.data.TensorDataset(val_embedd_tensor, y_val_one_hot_tensor)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n     \n\n# Initialize early stopping\nbest_val_loss = float('inf')\ncounter = 0\n\n# Training loop\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0.0\n    train_acc = 0.0\n\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, torch.argmax(labels, dim=1))  # Assuming labels are one-hot encoded\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        train_acc += torch.sum(preds == torch.argmax(labels, dim=1)).item()\n\n    train_loss = train_loss / len(train_loader.dataset)\n    train_acc = train_acc / len(train_loader.dataset)\n\n    model.eval()\n    val_loss = 0.0\n    val_acc = 0.0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, torch.argmax(labels, dim=1))  # Assuming labels are one-hot encoded\n\n            val_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            val_acc += torch.sum(preds == torch.argmax(labels, dim=1)).item()\n\n    val_loss = val_loss / len(val_loader.dataset)\n    val_acc = val_acc / len(val_loader.dataset)\n\n    print(f'Epoch [{epoch+1}/{EPOCHS}] - '\n          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\n    # Check for early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"Training completed.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:09:42.298263Z","iopub.execute_input":"2023-08-22T17:09:42.298632Z","iopub.status.idle":"2023-08-22T17:13:58.355292Z","shell.execute_reply.started":"2023-08-22T17:09:42.298601Z","shell.execute_reply":"2023-08-22T17:13:58.353700Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"GRUModel(\n  (gru): LRPGRU(768, 256, batch_first=True)\n  (dropout): Dropout(p=0.011691195516528197, inplace=False)\n  (dense): LRPLinear(in_features=256, out_features=8, bias=True)\n  (softmax): Softmax(dim=1)\n)\nEpoch [1/50] - Train Loss: 1.8439, Train Acc: 0.4302, Val Loss: 1.7108, Val Acc: 0.5536\nEpoch [2/50] - Train Loss: 1.7098, Train Acc: 0.5639, Val Loss: 1.7220, Val Acc: 0.5483\nEpoch [3/50] - Train Loss: 1.6628, Train Acc: 0.6108, Val Loss: 1.6559, Val Acc: 0.6172\nEpoch [4/50] - Train Loss: 1.6201, Train Acc: 0.6535, Val Loss: 1.6183, Val Acc: 0.6523\nEpoch [5/50] - Train Loss: 1.5978, Train Acc: 0.6778, Val Loss: 1.6009, Val Acc: 0.6702\nEpoch [6/50] - Train Loss: 1.5788, Train Acc: 0.6965, Val Loss: 1.6153, Val Acc: 0.6616\nEpoch [7/50] - Train Loss: 1.5717, Train Acc: 0.7012, Val Loss: 1.5881, Val Acc: 0.6854\nEpoch [8/50] - Train Loss: 1.5463, Train Acc: 0.7295, Val Loss: 1.5852, Val Acc: 0.6868\nEpoch [9/50] - Train Loss: 1.5361, Train Acc: 0.7398, Val Loss: 1.5752, Val Acc: 0.7026\nEpoch [10/50] - Train Loss: 1.5249, Train Acc: 0.7526, Val Loss: 1.5862, Val Acc: 0.6881\nEpoch [11/50] - Train Loss: 1.5179, Train Acc: 0.7573, Val Loss: 1.5773, Val Acc: 0.6940\nEpoch [12/50] - Train Loss: 1.5113, Train Acc: 0.7642, Val Loss: 1.5698, Val Acc: 0.7020\nEpoch [13/50] - Train Loss: 1.5124, Train Acc: 0.7628, Val Loss: 1.5832, Val Acc: 0.6881\nEpoch [14/50] - Train Loss: 1.5084, Train Acc: 0.7674, Val Loss: 1.5665, Val Acc: 0.7046\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, torch\u001b[38;5;241m.\u001b[39margmax(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Assuming labels are one-hot encoded\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     99\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Test the model on test data and compute accuracy\ntest_embedd_tensor = torch.tensor(test_embeddings, dtype=torch.float32)\ny_test_one_hot_tensor = torch.tensor(y_test_one_hot, dtype=torch.float32)\ntest_dataset = torch.utils.data.TensorDataset(test_embedd_tensor, y_test_one_hot_tensor)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel.eval()\ntest_acc = 0.0\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        test_acc += torch.sum(preds == torch.argmax(labels, dim=1)).item()\n\ntest_acc = test_acc / len(test_loader.dataset)\nprint(f'Test Accuracy: {test_acc:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:15:57.327170Z","iopub.execute_input":"2023-08-22T17:15:57.328255Z","iopub.status.idle":"2023-08-22T17:15:58.896339Z","shell.execute_reply.started":"2023-08-22T17:15:57.328203Z","shell.execute_reply":"2023-08-22T17:15:58.895309Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.6907\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model state dictionary\ntorch.save(model.state_dict(), 'model_state.pth')\n\nmodel.load_state_dict(torch.load('model_state.pth'))\nmodel.eval()  # Set the model to evaluation mode\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:16:14.534456Z","iopub.execute_input":"2023-08-22T17:16:14.535429Z","iopub.status.idle":"2023-08-22T17:16:14.555051Z","shell.execute_reply.started":"2023-08-22T17:16:14.535379Z","shell.execute_reply":"2023-08-22T17:16:14.554128Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"GRUModel(\n  (gru): LRPGRU(768, 256, batch_first=True)\n  (dropout): Dropout(p=0.011691195516528197, inplace=False)\n  (dense): LRPLinear(in_features=256, out_features=8, bias=True)\n  (softmax): Softmax(dim=1)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef explain_lrp(model, x):\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        # LRP forward\n        model.dense.attr()\n        model.gru.attr()\n    \n\n        gru_out= model.gru(x)\n        print(\"gru_out shape:\", gru_out.shape)\n        \n        h_final = np.concatenate((gru_out[:, -1, :model.gru.hidden_size],\n                                  gru_out[:, 0, model.gru.hidden_size:]), axis=-1)\n        h_final_reshaped = h_final.reshape(-1, model.dense.weight.shape[1])\n        output = model.dense(h_final_reshaped)\n\n        # LRP backward\n        target_class = np.argmax(output, axis=1)\n        rel_output = np.zeros_like(output)\n        rel_output[np.arange(output.shape[0]), target_class] = output[np.arange(output.shape[0]), target_class]\n        rel_h_final = model.dense.attr_backward(rel_output)\n        print(\"rel_h_final shape:\", rel_h_final.shape)\n\n        rel_gru_out = np.zeros(gru_out.shape)\n        rel_gru_out[:, -1, :model.gru.hidden_size] = rel_h_final[:, :model.gru.hidden_size]\n        rel_gru_out[:, 0, model.gru.hidden_size:] = rel_h_final[:, model.gru.hidden_size:]\n        print(\"rel_gru_out shape:\", rel_gru_out.shape)\n\n        rel_embeddings = model.gru.attr_backward(rel_gru_out)\n        print(\"rel_embeddings shape:\", rel_embeddings.shape)\n\n        return rel_embeddings","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:25:13.169253Z","iopub.execute_input":"2023-08-22T17:25:13.169617Z","iopub.status.idle":"2023-08-22T17:25:13.181600Z","shell.execute_reply.started":"2023-08-22T17:25:13.169587Z","shell.execute_reply":"2023-08-22T17:25:13.180559Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def exp_lrp(idx):\n    rel_embeddings = explain_lrp(model, test_embedd_tensor[idx].unsqueeze(0))\n    print(\"tweet:\",X_test.iloc[idx])\n\n    print(\"Relevance Scores:\",rel_embeddings.sum(axis=-1).squeeze())\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:32:35.226132Z","iopub.execute_input":"2023-08-22T17:32:35.226511Z","iopub.status.idle":"2023-08-22T17:32:35.233811Z","shell.execute_reply.started":"2023-08-22T17:32:35.226480Z","shell.execute_reply":"2023-08-22T17:32:35.231411Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"exp_lrp(15)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T17:32:44.015194Z","iopub.execute_input":"2023-08-22T17:32:44.015562Z","iopub.status.idle":"2023-08-22T17:32:44.126334Z","shell.execute_reply.started":"2023-08-22T17:32:44.015531Z","shell.execute_reply":"2023-08-22T17:32:44.125257Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"gru_out shape: (1, 50, 256)\nrel_h_final shape: (1, 256)\nrel_gru_out shape: (1, 50, 256)\nrel_embeddings shape: (1, 50, 768)\ntweet: وائل كفوري عششق اتم اتخلص لادخل الجنه حزن حزن\nRelevance Scores: [-4.04708457e+137 -3.77786629e+136  4.49210194e+132  1.25454984e+129\n  4.56352245e+126 -1.63260012e+124 -5.99873596e+120 -5.78387441e+115\n  2.69525656e+113 -1.49107899e+110  1.87941378e+108  4.44622911e+104\n  1.17099977e+101  1.03990277e+099 -6.14764155e+095  7.02825820e+092\n  5.71021996e+089  4.26022178e+086  1.38734703e+083 -7.04282274e+080\n -4.49513870e+076  8.61103613e+074  2.03835123e+072  1.96130804e+069\n  2.49287257e+066  3.23507284e+063  1.22859278e+060  9.05836461e+056\n  7.67934785e+053  2.96605171e+050  1.12666020e+047 -1.43721184e+044\n -1.07424247e+041 -8.08029147e+037 -1.56871004e+035 -2.47341465e+032\n  2.61253768e+029  7.32854496e+025  4.65241786e+022  5.82225178e+019\n  6.59404850e+016  6.64435084e+013  3.37987767e+010 -8.58932407e+007\n -4.49844681e+005 -1.45368007e+003 -4.11898041e+000  2.31066879e-001\n  1.78434307e+000  6.88790832e-001]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}