{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T15:03:00.474621Z",
     "iopub.status.busy": "2023-08-31T15:03:00.474209Z",
     "iopub.status.idle": "2023-08-31T15:03:14.947654Z",
     "shell.execute_reply": "2023-08-31T15:03:14.946161Z",
     "shell.execute_reply.started": "2023-08-31T15:03:00.474585Z"
    },
    "id": "FtXnDFfR4_2b",
    "outputId": "4e6de28f-8ba8-4e49-b0c3-21acba14110f"
   },
   "outputs": [],
   "source": [
    "%pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = '1-DTIOsUZVbmiGJZMMmLbcSN_NWuqSbql'\n",
    "val = '1-8B6g2l8D9U_O370fv6a0O7o_pozyrx0'\n",
    "test = '1-FAL4G--bLerOPdtHWoUb2FelHAc-cRN'\n",
    "\n",
    "cleaned_train_embedding = '1--R-xfUHNI4XifRJlqOFQ_VUqlJ59rk6'\n",
    "cleaned_val_embedding = '1-192wuFcIa3Gu1uHl4HP_Itg7It4_8lC'\n",
    "cleaned_test_embedding = '1_gGH6CyYS0QXr2pAjd_nehu3WW3Bdpgu'\n",
    "\n",
    "model_path = '1XfwX7_t0_W1vxFizbpzYsnaDn6S900QU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown {train}\n",
    "!gdown {val}\n",
    "!gdown {test}\n",
    "\n",
    "!gdown {cleaned_train_embedding}\n",
    "!gdown {cleaned_val_embedding}\n",
    "!gdown {cleaned_test_embedding}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:07:21.094071Z",
     "iopub.status.busy": "2023-08-31T14:07:21.093371Z",
     "iopub.status.idle": "2023-08-31T14:07:21.107176Z",
     "shell.execute_reply": "2023-08-31T14:07:21.106129Z",
     "shell.execute_reply.started": "2023-08-31T14:07:21.094030Z"
    },
    "id": "cLmXB__I1uOg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import BertModel, AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:07:43.827540Z",
     "iopub.status.busy": "2023-08-31T14:07:43.827144Z",
     "iopub.status.idle": "2023-08-31T14:07:43.832979Z",
     "shell.execute_reply": "2023-08-31T14:07:43.831548Z",
     "shell.execute_reply.started": "2023-08-31T14:07:43.827509Z"
    }
   },
   "outputs": [],
   "source": [
    "label_to_class = {\n",
    "    0: 'none',\n",
    "    1: 'anger',\n",
    "    2: 'joy',\n",
    "    3: 'sadness',\n",
    "    4: 'love',\n",
    "    5: 'sympathy',\n",
    "    6: 'surprise',\n",
    "    7: 'fear'\n",
    "}\n",
    "classes = ['none', 'anger', 'joy', 'sadness', 'love', 'sympathy', 'surprise', 'fear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:07:50.003274Z",
     "iopub.status.busy": "2023-08-31T14:07:50.002899Z",
     "iopub.status.idle": "2023-08-31T14:07:52.291859Z",
     "shell.execute_reply": "2023-08-31T14:07:52.290630Z",
     "shell.execute_reply.started": "2023-08-31T14:07:50.003240Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/working/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "with open('/kaggle/working/val.pkl', 'rb') as f:\n",
    "    val = pickle.load(f)\n",
    "\n",
    "with open('/kaggle/working/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "with open('/kaggle/working/ls_train_embeddings.pkl', 'rb') as f:\n",
    "    train_embeddings = pickle.load(f)\n",
    "    \n",
    "with open('/kaggle/working/ls_val_embeddings.pkl', 'rb') as f:\n",
    "    val_embeddings = pickle.load(f)\n",
    "    \n",
    "with open('/kaggle/working/ls_test_embeddings.pkl', 'rb') as f:\n",
    "    test_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:08:20.416958Z",
     "iopub.status.busy": "2023-08-31T14:08:20.416464Z",
     "iopub.status.idle": "2023-08-31T14:08:20.441528Z",
     "shell.execute_reply": "2023-08-31T14:08:20.440445Z",
     "shell.execute_reply.started": "2023-08-31T14:08:20.416917Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "\n",
    "y_train = encoder.fit_transform(train['label'].values.reshape(-1,1)).toarray()\n",
    "y_val = encoder.transform(val['label'].values.reshape(-1,1)).toarray()\n",
    "y_test = encoder.transform(test['label'].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:08:31.679238Z",
     "iopub.status.busy": "2023-08-31T14:08:31.678866Z",
     "iopub.status.idle": "2023-08-31T14:08:31.684148Z",
     "shell.execute_reply": "2023-08-31T14:08:31.683120Z",
     "shell.execute_reply.started": "2023-08-31T14:08:31.679206Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_embeddings\n",
    "X_val = val_embeddings\n",
    "X_test = test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNV13COwKUd5",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "EMBED_SIZE = 86#768#86\n",
    "LEARNING_RATE =  0.006535000000000002\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='max',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_gru(embedding_layer=False):\n",
    "    model = Sequential()\n",
    "    if embedding_layer:\n",
    "        model.add(Embedding(VOCAB_SIZE_MARBERT, EMBED_SIZE, input_length=max_length, trainable=True))#, weights=[embedding_matrix.detach().numpy()]))\n",
    "        model.add(GRU(128, return_sequences=True))\n",
    "    else:\n",
    "        model.add(GRU(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(64, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(LEARNING_RATE),\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = build_gru(embedding_layer=False)\n",
    "\n",
    "history = model.fit(X_train, np.asarray(y_train), validation_data=(X_val, np.asarray(y_val)), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:08:44.097915Z",
     "iopub.status.busy": "2023-08-31T14:08:44.097543Z",
     "iopub.status.idle": "2023-08-31T14:08:58.100953Z",
     "shell.execute_reply": "2023-08-31T14:08:58.099870Z",
     "shell.execute_reply.started": "2023-08-31T14:08:44.097884Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"GRRU\")\n",
    "model = keras.models.load_model(\"/kaggle/input/modelgru/kaggle/working/GRRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:08:58.120609Z",
     "iopub.status.busy": "2023-08-31T14:08:58.120280Z",
     "iopub.status.idle": "2023-08-31T14:09:04.341951Z",
     "shell.execute_reply": "2023-08-31T14:09:04.340803Z",
     "shell.execute_reply.started": "2023-08-31T14:08:58.120578Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test.argmax(axis=1), predictions.argmax(axis=1), target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T05:30:34.606827Z",
     "iopub.status.busy": "2023-08-30T05:30:34.606169Z",
     "iopub.status.idle": "2023-08-30T05:30:35.641639Z",
     "shell.execute_reply": "2023-08-30T05:30:35.640678Z",
     "shell.execute_reply.started": "2023-08-30T05:30:34.606781Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def map_label_to_class(indices, classes):\n",
    "  return [classes[idx] for idx in indices]\n",
    "\n",
    "def create_csv(y_pred_idx, y_true_idx, file_path=\"examples.csv\"):\n",
    "  y_pred, y_true = map_label_to_class(y_pred_idx, label_to_class), map_label_to_class(y_true_idx, label_to_class)\n",
    "  with open(file_path, 'w') as file:\n",
    "    file.write('Index,True Label,Predicted Label\\n')\n",
    "    for i in range(len(y_pred)):\n",
    "      file.write(f'{i},{y_true[i]},{y_pred[i]}\\n')\n",
    "  print(\"Content has been written to the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "create_csv(predictions.argmax(axis=1), y_test.argmax(axis=1), file_path=\"gru.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "misclassified = np.where(y_test.argmax(axis=1)!=predictions.argmax(axis=1))[0]\n",
    "correctly_classified = np.where(y_test.argmax(axis=1)==predictions.argmax(axis=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(misclassified), len(correctly_classified), len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "problematic_samples = {'predicted':['none', 'none', 'joy', 'anger', 'love', 'none'], 'actual':['joy', 'surprise', 'love', 'sadness', 'joy', 'sadness']}\n",
    "problematic_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for predicted, actual in zip(problematic_samples['predicted'], problematic_samples['actual']):\n",
    "    tmp = []\n",
    "    for idx, data in enumerate(zip(predictions.argmax(axis=1), y_test.argmax(axis=1))):\n",
    "        pred, true = data\n",
    "        if classes[pred]==predicted and classes[true]==actual:\n",
    "            tmp.append(idx)\n",
    "    problematic_indices.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sum([len(x) for x in problematic_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:27:36.147223Z",
     "iopub.status.busy": "2023-08-31T14:27:36.146706Z",
     "iopub.status.idle": "2023-08-31T14:27:51.899593Z",
     "shell.execute_reply": "2023-08-31T14:27:51.898598Z",
     "shell.execute_reply.started": "2023-08-31T14:27:36.147181Z"
    }
   },
   "outputs": [],
   "source": [
    "marbert_model_path = 'UBC-NLP/MARBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(marbert_model_path, from_tf=True)\n",
    "marbert_model = TFAutoModel.from_pretrained(marbert_model_path, output_hidden_states=True)\n",
    "\n",
    "remove_special_tokens=0  #change this to 0 if you want to keep the special token\n",
    "\n",
    "test_df = test.reset_index()\n",
    "test_true = y_test.argmax(axis=1)\n",
    "test_pred = predictions.argmax(axis=1)\n",
    "\n",
    "def bert_tokenize(text: str) -> dict:\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=50)\n",
    "    if remove_special_tokens == 1:\n",
    "        shape = np.array(tokens['input_ids']).shape\n",
    "        modified_input_ids = np.zeros(shape).astype(np.int32)\n",
    "        modified_attention_mask = np.zeros(shape).astype(np.int32)\n",
    "        # Modify the input IDs and attention mask as per your requirement\n",
    "        for i in range(shape[0]):\n",
    "            modified_input_ids[i] = [0 if token_id == 1 else 0 if token_id == 3 else 0 if token_id == 0 else 0 if token_id == 2 else 0 if token_id == 4 else token_id for token_id in tokens['input_ids'][i]]\n",
    "            modified_attention_mask[i] = [0 if token_id in [1, 3, 0, 2, 4] else 1 for token_id in tokens['input_ids'][i]]\n",
    "        # Update the input IDs and attention mask in the tokens dictionary\n",
    "        tokens['input_ids'] = modified_input_ids\n",
    "        tokens['attention_mask'] = modified_attention_mask\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_embeddings(ids, mask, type_ids):\n",
    "    ids = tf.convert_to_tensor(ids)\n",
    "    #print(ids.shape)\n",
    "    mask = tf.convert_to_tensor(mask)\n",
    "    #print(mask.shape)\n",
    "    #print(mask)\n",
    "    type_ids = tf.convert_to_tensor(type_ids)\n",
    "    #print(type_ids.shape)\n",
    "    hidden_states = marbert_model(input_ids=ids, attention_mask=mask, token_type_ids=type_ids)[0]\n",
    "    averaged_embedding = tf.reduce_mean(hidden_states, axis=1)\n",
    "    return hidden_states.numpy()\n",
    "\n",
    "def embedd(text):\n",
    "    if type(text)==type(pd.Series()):\n",
    "        text=text.values.astype(str).tolist()\n",
    "    tokens = bert_tokenize(text)\n",
    "    xlen = np.array(tokens['input_ids']).shape[0]\n",
    "    x_emb = np.zeros((xlen,50,768))\n",
    "    for i in range(0,xlen,100):\n",
    "        if(i+100 < xlen):\n",
    "            input_ids = tokens['input_ids'][i:i+100]\n",
    "            attention_mask = tokens['attention_mask'][i:i+100]\n",
    "            token_type_ids = tokens['token_type_ids'][i:i+100]\n",
    "            x_emb[i:i+100] = get_embeddings(input_ids,attention_mask,token_type_ids)\n",
    "        else:\n",
    "            input_ids = tokens['input_ids'][i:xlen]\n",
    "            attention_mask = tokens['attention_mask'][i:xlen]\n",
    "            token_type_ids = tokens['token_type_ids'][i:xlen]\n",
    "            x_emb[i:xlen] = get_embeddings(input_ids,attention_mask,token_type_ids)\n",
    "    return x_emb\n",
    "\n",
    "def model_predict(texts):\n",
    "    embedding = embedd(texts)\n",
    "    return model.predict(embedding, verbose=0)\n",
    "\n",
    "# Create a LimeTextExplainer\n",
    "lime_explainer = LimeTextExplainer(class_names=classes)\n",
    "\n",
    "def lime_explain(idx):\n",
    "    text = test_df['Light Stemming'][idx]\n",
    "    if len(text.split()) < 1:\n",
    "        print(\"Text contains less than 2 words. Cannot explain.\")\n",
    "        return\n",
    "    # Explain the specific prediction\n",
    "    if test_true[idx]==test_pred[idx]:\n",
    "        labels_to_explain = (test_true[idx],)\n",
    "    else:\n",
    "        labels_to_explain = (test_true[idx], test_pred[idx])\n",
    "    explanation = lime_explainer.explain_instance(test_df['Light Stemming'][idx], model_predict, labels=labels_to_explain)\n",
    "    # Show the explanation\n",
    "    explanation.show_in_notebook()\n",
    "\n",
    "\n",
    "def shap_model_predict(text):\n",
    "  text = text.astype(str).tolist()\n",
    "  tokens = bert_tokenize(text)\n",
    "  xlen = np.array(tokens['input_ids']).shape[0]\n",
    "  x_emb = np.zeros((xlen,50,768))\n",
    "  for i in range(0,xlen,100):\n",
    "    if(i+100 < xlen):\n",
    "        input_ids = tokens['input_ids'][i:i+100]\n",
    "        attention_mask = tokens['attention_mask'][i:i+100]\n",
    "        token_type_ids = tokens['token_type_ids'][i:i+100]\n",
    "        x_emb[i:i+100] = get_embeddings(input_ids,attention_mask,token_type_ids)\n",
    "    else:\n",
    "        input_ids = tokens['input_ids'][i:xlen]\n",
    "        attention_mask = tokens['attention_mask'][i:xlen]\n",
    "        token_type_ids = tokens['token_type_ids'][i:xlen]\n",
    "        x_emb[i:xlen] = get_embeddings(input_ids,attention_mask,token_type_ids)\n",
    "  return model.predict(x_emb, verbose=0)\n",
    "\n",
    "\n",
    "masker = shap.maskers.Text(tokenizer=r\"\\W+\")\n",
    "shap_explainer = shap.Explainer(shap_model_predict, output_names=classes, masker=masker)\n",
    "\n",
    "\n",
    "def deep_shap_explain(idx):\n",
    "    instance = test_df['Light Stemming'][idx:idx+1].values.astype(str).tolist()\n",
    "    if len(instance[0].split()) < 2:\n",
    "        print(\"Text contains less than 2 words. Cannot explain.\")\n",
    "        return\n",
    "    # Assuming 'explainer' is a function that computes SHAP values for the instance\n",
    "    shap_values = shap_explainer(instance)  # You need to replace 'explainer' with the actual SHAP explainer function\n",
    "    shap_values = shap_values[0,:,test_pred[idx]]\n",
    "    # Assuming 'shap' is the SHAP library\n",
    "#     shap.plots.bar(shap_values)  # Display a summary plot of SHAP values\n",
    "    shap.text_plot(shap_values)\n",
    "    \n",
    "    # Print all SHAP values for the instance\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    \n",
    "def print_sample(idx):\n",
    "    print('Index: ', idx)\n",
    "    print('True label: ', classes[test_true[idx]])\n",
    "    print('Predicted label: ', classes[test_pred[idx]])\n",
    "    print(\"Original tweet:\", test_df['tweet'][idx])\n",
    "    print(\"Cleaned tweet:\", test_df['Light Stemming'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pair_idx, indices in enumerate(problematic_indices):\n",
    "    print()\n",
    "    print()\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print('Predicted: ', problematic_samples['predicted'][pair_idx])\n",
    "    print('Actual: ', problematic_samples['actual'][pair_idx])\n",
    "    for idx in indices:\n",
    "        print('Index: ', idx)\n",
    "        print('Tweet: ', test['tweet'].values.tolist()[idx])\n",
    "        print('Light stemming: ', test['Light Stemming'].values.tolist()[idx])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T07:07:00.086506Z",
     "iopub.status.busy": "2023-08-30T07:07:00.085961Z",
     "iopub.status.idle": "2023-08-30T07:07:00.095836Z",
     "shell.execute_reply": "2023-08-30T07:07:00.093878Z",
     "shell.execute_reply.started": "2023-08-30T07:07:00.086470Z"
    }
   },
   "outputs": [],
   "source": [
    "def lime_explain_with_text(idx,text):\n",
    "    # Explain the specific prediction\n",
    "    if test_true[idx]==test_pred[idx]:\n",
    "        labels_to_explain = (test_true[idx],)\n",
    "    else:\n",
    "        labels_to_explain = (test_true[idx], test_pred[idx])\n",
    "    explanation = lime_explainer.explain_instance(text, model_predict, labels=labels_to_explain)\n",
    "    # Show the explanation\n",
    "    explanation.show_in_notebook()\n",
    "\n",
    "\n",
    "def deep_shap_explain_with_text(idx,text):\n",
    "    instance = text#test_df['Light Stemming'][idx:idx+1].values.astype(str).tolist()\n",
    "    \n",
    "    # Assuming 'explainer' is a function that computes SHAP values for the instance\n",
    "    shap_values = shap_explainer(instance)  # You need to replace 'explainer' with the actual SHAP explainer function\n",
    "   # shap_values = shap_values[0,:,test_pred[idx]]\n",
    "    # Assuming 'shap' is the SHAP library\n",
    "#     shap.plots.bar(shap_values)  # Display a summary plot of SHAP values\n",
    "    shap.text_plot(shap_values)\n",
    "    \n",
    "    # Print all SHAP values for the instance\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "short_sentence_indices = test_df[test_df['Light Stemming'].apply(lambda x: len(x.split())) < 4].index.to_numpy()\n",
    "long_sentence_indices = test_df[test_df['Light Stemming'].apply(lambda x: len(x.split())) > 20].index.to_numpy()\n",
    "\n",
    "print(\"Indices of short sentences:\", short_sentence_indices)\n",
    "print(\"Indices of long sentences:\", long_sentence_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Short sentences with correct classification\n",
    "print(\"Correctly Classified Short Sentences:\")\n",
    "correct_short_indices = [idx for idx in short_sentence_indices if test_true[idx] == test_pred[idx]][:]  # Print only 5 samples\n",
    "print(len(correct_short_indices))\n",
    "for idx in correct_short_indices:\n",
    "    print_sample(idx)\n",
    "    print()\n",
    "\n",
    "# Long sentences with correct classification\n",
    "print(\"Correctly Classified Long Sentences:\")\n",
    "correct_long_indices = [idx for idx in long_sentence_indices if test_true[idx] == test_pred[idx]][:]  # Print only 5 samples\n",
    "\n",
    "print(len(correct_long_indices))\n",
    "for idx in correct_long_indices:\n",
    "    print_sample(idx)\n",
    "    print()\n",
    "\n",
    "# Short sentences with misclassification\n",
    "print(\"Misclassified Short Sentences:\")\n",
    "\n",
    "misclassified_short_indices = [idx for idx in short_sentence_indices if test_true[idx] != test_pred[idx]][:]  # Print only 5 samples\n",
    "print(len(misclassified_short_indices))\n",
    "\n",
    "for idx in misclassified_short_indices:\n",
    "    print_sample(idx)\n",
    "    print()\n",
    "\n",
    "# Long sentences with misclassification\n",
    "print(\"Misclassified Long Sentences:\")\n",
    "misclassified_long_indices = [idx for idx in long_sentence_indices if test_true[idx] != test_pred[idx]][:]  # Print only 5 samples\n",
    "print(len(misclassified_long_indices))\n",
    "\n",
    "for idx in misclassified_long_indices:\n",
    "    print_sample(idx)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_correct_short = len(correct_short_indices)\n",
    "num_correct_long = len(correct_long_indices)\n",
    "num_misclassified_short = len(misclassified_short_indices)\n",
    "num_misclassified_long = len(misclassified_long_indices)\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Category                                        Correct    Misclassified\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Short Sentences less than 4 words/sentence:     {:<12}     {:<12}\".format(num_correct_short, num_misclassified_short))\n",
    "print(\"Long Sentences more than 20 words/sentence:     {:<12}     {:<12}\".format(num_correct_long, num_misclassified_long))\n",
    "print(\"------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T15:04:27.286660Z",
     "iopub.status.busy": "2023-08-31T15:04:27.286230Z",
     "iopub.status.idle": "2023-08-31T15:04:27.293907Z",
     "shell.execute_reply": "2023-08-31T15:04:27.292799Z",
     "shell.execute_reply.started": "2023-08-31T15:04:27.286617Z"
    }
   },
   "outputs": [],
   "source": [
    "file ='/kaggle/working/newWords.txt'\n",
    "def searchInFile(searchWord, filename=file):\n",
    "  with open(filename, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "      lineWords = line.split(\" \")\n",
    "      # to remove white spaces from the word\n",
    "      if searchWord.replace(\" \", \"\") == lineWords[1]:\n",
    "        print(line)\n",
    "        return\n",
    "    print(\"Word exists in training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T15:10:30.721919Z",
     "iopub.status.busy": "2023-08-31T15:10:30.721371Z",
     "iopub.status.idle": "2023-08-31T15:10:30.735173Z",
     "shell.execute_reply": "2023-08-31T15:10:30.733804Z",
     "shell.execute_reply.started": "2023-08-31T15:10:30.721877Z"
    }
   },
   "outputs": [],
   "source": [
    "searchInFile('شو')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T15:13:04.034795Z",
     "iopub.status.busy": "2023-08-31T15:13:04.034350Z",
     "iopub.status.idle": "2023-08-31T15:22:20.886616Z",
     "shell.execute_reply": "2023-08-31T15:22:20.885489Z",
     "shell.execute_reply.started": "2023-08-31T15:13:04.034757Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in [40,108,117,175,225,390,467,807,838,872,920,975,989,1065,1088,1415,1439,1472]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relgoius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T14:29:09.290751Z",
     "iopub.status.busy": "2023-08-31T14:29:09.290128Z",
     "iopub.status.idle": "2023-08-31T14:36:46.872439Z",
     "shell.execute_reply": "2023-08-31T14:36:46.870808Z",
     "shell.execute_reply.started": "2023-08-31T14:29:09.290688Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in [102,156,286,295,372,407,656,756,782,854,957,981,1279,1293,1387,1508]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## praying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T15:24:51.013126Z",
     "iopub.status.busy": "2023-08-31T15:24:51.012641Z",
     "iopub.status.idle": "2023-08-31T15:31:26.874573Z",
     "shell.execute_reply": "2023-08-31T15:31:26.873512Z",
     "shell.execute_reply.started": "2023-08-31T15:24:51.013092Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in [150,172,173,176,186,615,677,699,705,725,1207,1247,1252,1503]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-romantic love"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T15:33:09.799535Z",
     "iopub.status.busy": "2023-08-31T15:33:09.798905Z",
     "iopub.status.idle": "2023-08-31T15:38:16.923354Z",
     "shell.execute_reply": "2023-08-31T15:38:16.922246Z",
     "shell.execute_reply.started": "2023-08-31T15:33:09.799491Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in [116,125,141,146,174,178,227,281,307,324,325]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T15:41:40.625341Z",
     "iopub.status.busy": "2023-08-31T15:41:40.624887Z",
     "iopub.status.idle": "2023-08-31T15:47:57.982931Z",
     "shell.execute_reply": "2023-08-31T15:47:57.981692Z",
     "shell.execute_reply.started": "2023-08-31T15:41:40.625306Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in [136,139,167,184,201,879,881,901,908,1423,1447,1480,149]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## repeated tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in [23,260,932]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 260\n",
    "text = 'هنجيب ميداليه الاوليمبياد ياعماد  حزن حزن حزن حزن حزن حزن حزن حزن'\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 11\n",
    "text = 'سخريه سخريه سخريه سخريه كنت يدخل العشق قلبه يبصر جفونك يعشق تصميمي تصميم رمزي جده الخير الخير سخريه سخريه سخريه  سخريه  سخريه'\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 260\n",
    "text = 'هنجيب ميداليه الاوليمبياد ياعماد  فرح'\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 932\n",
    "text = 'شو مشتاقه تهدي باقه ورد صغيره  حب '\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 932\n",
    "text = 'شو مشتاقه تهدي باقه ورد صغيره حب حب حب حب حب حب '\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 23\n",
    "text = 'جايز ومم بالعوامه البطه  الجاي سخريه'\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in [75, 76, 100, 108, 130]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in [100, 108, 130]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in[ 656,1508,169,782]:\n",
    "    print_sample(index)\n",
    "    lime_explain(index)\n",
    "    deep_shap_explain(index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 727\n",
    "text = 'عرف يبقي خير معرفناش يبقي لا يكلف الله نفسا الا وسع فين ايام كرم حابر ميداليه جيب'\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in misclassified_short_indices:\n",
    "     print_sample(index)\n",
    "for index in misclassified_short_indices:\n",
    "     print_sample(index)\n",
    "     lime_explain(index)\n",
    "     deep_shap_explain(index)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 727\n",
    "text = 'عرف يبقي خير معرفناش يبقي لا يكلف الله نفسا الا وسع فين ايام كرم حابر ميداليه جيب'\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 469\n",
    "text = 'ولقيت مايكل فيليبس بيكسب ذهبي ربع ساعه وحاجه معلش هكتب التاريخ وراجع تاني'\n",
    "print([text])\n",
    "print(text)\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,text)\n",
    "deep_shap_explain_with_text(idx,[text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining manually selected samples for each problematic block using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =1198 \n",
    "text = test_df['tweet'][idx:idx+1].values.astype(str).tolist()\n",
    "print_sample(idx)\n",
    "lime_explain_with_text(idx,test_df['tweet'][idx])\n",
    "deep_shap_explain_with_text(idx,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 307\n",
    "print_sample(sample)\n",
    "lime_explain(sample)\n",
    "deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 1228\n",
    "print_sample(sample)\n",
    "lime_explain(sample)\n",
    "deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 5\n",
    "print_sample(sample)\n",
    "lime_explain(sample)\n",
    "deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isseue in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in [1317,768]:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed feelings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 157\n",
    "print_sample(sample)\n",
    "lime_explain(sample)\n",
    "deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 1211\n",
    "print_sample(sample)\n",
    "lime_explain(sample)\n",
    "deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 214\n",
    "print_sample(sample)\n",
    "lime_explain(sample)\n",
    "deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_selected_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual: joy, Prediced: none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_selected_samples.append([2,23,189,534,1402,1200,294,298,181])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in [1338,904,1038,335,1228]:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual: surprise, Prediced: none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_selected_samples.append([154,1158,1014,848,811])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in manually_selected_samples[1]:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual: love, Prediced: joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_selected_samples.append([298,320,932,985,276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in manually_selected_samples[2]:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual: anger, Prediced: sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_selected_samples.append([114,1211,1309,1444,1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in [114,1211,1309,1444,1001]:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual: joy, Prediced: love"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_selected_samples.append([327,378,689,335,214])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in [327,378,689,335,214]:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual: sadness, Prediced: none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_selected_samples.append([254,742,1255,820,862])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in manually_selected_samples[5]:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples agreed on by all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_agreed_on = [3,2,1,429,43,354,56,391,44,263,32,172,17,1496,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples_agreed_on:\n",
    "    print_sample(sample)\n",
    "    lime_explain(sample)\n",
    "    deep_shap_explain(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
