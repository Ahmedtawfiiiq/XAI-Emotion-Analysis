{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install gdown","metadata":{"id":"FtXnDFfR4_2b","outputId":"4e6de28f-8ba8-4e49-b0c3-21acba14110f","execution":{"iopub.status.busy":"2023-08-10T22:46:50.665162Z","iopub.execute_input":"2023-08-10T22:46:50.666017Z","iopub.status.idle":"2023-08-10T22:47:04.632106Z","shell.execute_reply.started":"2023-08-10T22:46:50.665982Z","shell.execute_reply":"2023-08-10T22:47:04.630907Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown \"1Fs0PTy_xPsoX5bg4QTCrX6JAaQCHRzis\"\n!gdown \"1JLa-ELhUskQINi0syf3YJWBX8jcCXdn4\"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:04.634312Z","iopub.execute_input":"2023-08-10T22:47:04.635145Z","iopub.status.idle":"2023-08-10T22:47:08.949473Z","shell.execute_reply.started":"2023-08-10T22:47:04.635106Z","shell.execute_reply":"2023-08-10T22:47:08.948348Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1Fs0PTy_xPsoX5bg4QTCrX6JAaQCHRzis\nTo: /kaggle/working/train.pkl\n100%|███████████████████████████████████████| 4.09M/4.09M [00:00<00:00, 196MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1JLa-ELhUskQINi0syf3YJWBX8jcCXdn4\nTo: /kaggle/working/test.pkl\n100%|██████████████████████████████████████| 1.02M/1.02M [00:00<00:00, 95.2MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nimport string\nimport re\nimport pickle\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.isri import ISRIStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report\nfrom sklearn.svm import SVC\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import backend as K\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nimport torch\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"id":"cLmXB__I1uOg","execution":{"iopub.status.busy":"2023-08-10T22:47:08.952301Z","iopub.execute_input":"2023-08-10T22:47:08.952700Z","iopub.status.idle":"2023-08-10T22:47:21.929048Z","shell.execute_reply.started":"2023-08-10T22:47:08.952662Z","shell.execute_reply":"2023-08-10T22:47:21.928085Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"label_to_class = {\n    0: 'none',\n    1: 'anger',\n    2: 'joy',\n    3: 'sadness',\n    4: 'love',\n    5: 'sympathy',\n    6: 'surprise',\n    7: 'fear'\n}\nclasses = ['none', 'anger', 'joy', 'sadness', 'love', 'sympathy', 'surprise', 'fear']","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:21.931558Z","iopub.execute_input":"2023-08-10T22:47:21.932284Z","iopub.status.idle":"2023-08-10T22:47:21.939000Z","shell.execute_reply.started":"2023-08-10T22:47:21.932247Z","shell.execute_reply":"2023-08-10T22:47:21.937231Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/train.pkl', 'rb') as f:\n    train = pickle.load(f)\n\nwith open('/kaggle/working/test.pkl', 'rb') as f:\n    test = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:21.940119Z","iopub.execute_input":"2023-08-10T22:47:21.940707Z","iopub.status.idle":"2023-08-10T22:47:21.981267Z","shell.execute_reply.started":"2023-08-10T22:47:21.940672Z","shell.execute_reply":"2023-08-10T22:47:21.980398Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train = train['Light Stemming']\ny_train = train['label']\nX_test = test['Light Stemming']\ny_test = test['label']","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:21.982428Z","iopub.execute_input":"2023-08-10T22:47:21.983337Z","iopub.status.idle":"2023-08-10T22:47:21.990418Z","shell.execute_reply.started":"2023-08-10T22:47:21.983298Z","shell.execute_reply":"2023-08-10T22:47:21.989383Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"encoder = OneHotEncoder()\n\ny_train = encoder.fit_transform(y_train.values.reshape(-1,1)).toarray()\ny_test = encoder.transform(y_test.values.reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:21.992088Z","iopub.execute_input":"2023-08-10T22:47:21.992413Z","iopub.status.idle":"2023-08-10T22:47:22.012994Z","shell.execute_reply.started":"2023-08-10T22:47:21.992383Z","shell.execute_reply":"2023-08-10T22:47:22.012008Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def prepare_tokenization(train, val, test=None, pad=500):\n  tokenizer = Tokenizer(oov_token='<UNK>')\n  tokenizer.fit_on_texts(train)\n  tokenizer.word_index['<PAD>'] = 0\n  train = tokenizer.texts_to_sequences(train)\n  val = tokenizer.texts_to_sequences(val)\n  if not pad==False:\n    train = sequence.pad_sequences(train, maxlen=pad)\n    val = sequence.pad_sequences(val, maxlen=pad)\n  if not type(test)==type(None):\n    test = tokenizer.texts_to_sequences(test)\n    if not pad==False:\n      test = sequence.pad_sequences(test, maxlen=pad)\n  v_size = len(tokenizer.word_index)\n  print(\"Vocabulary size={}\".format(v_size))\n  print(\"Number of Documents={}\".format(tokenizer.document_count))\n  if not type(test)==type(None):\n    return v_size, train, val, test\n  else:\n    return v_size, train, val","metadata":{"id":"bvPIxCa3UZI8","execution":{"iopub.status.busy":"2023-08-10T22:47:22.014375Z","iopub.execute_input":"2023-08-10T22:47:22.014707Z","iopub.status.idle":"2023-08-10T22:47:22.023540Z","shell.execute_reply.started":"2023-08-10T22:47:22.014675Z","shell.execute_reply":"2023-08-10T22:47:22.022628Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"VOCAB_SIZE, train_sequences, test_sequences = prepare_tokenization(X_train, X_test, pad=False)","metadata":{"id":"SvzRM2xtah_w","outputId":"94e8465f-4e8e-4065-b6b0-d0d6ce8678fd","execution":{"iopub.status.busy":"2023-08-10T22:47:22.024910Z","iopub.execute_input":"2023-08-10T22:47:22.025540Z","iopub.status.idle":"2023-08-10T22:47:22.439010Z","shell.execute_reply.started":"2023-08-10T22:47:22.025506Z","shell.execute_reply":"2023-08-10T22:47:22.438084Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Vocabulary size=22650\nNumber of Documents=8052\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain_lens = [len(s) for s in train_sequences]\nplt.hist(train_lens)","metadata":{"id":"WV3GW9vPbshE","outputId":"8281e470-29b6-4dd0-89a0-43bb43dbf423","execution":{"iopub.status.busy":"2023-08-10T22:47:22.442491Z","iopub.execute_input":"2023-08-10T22:47:22.442789Z","iopub.status.idle":"2023-08-10T22:47:22.792723Z","shell.execute_reply.started":"2023-08-10T22:47:22.442760Z","shell.execute_reply":"2023-08-10T22:47:22.791810Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(array([5.460e+02, 1.762e+03, 1.954e+03, 1.314e+03, 1.684e+03, 7.210e+02,\n        6.700e+01, 3.000e+00, 0.000e+00, 1.000e+00]),\n array([ 0. ,  3.7,  7.4, 11.1, 14.8, 18.5, 22.2, 25.9, 29.6, 33.3, 37. ]),\n <BarContainer object of 10 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"max_length = max(train_lens)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:22.794795Z","iopub.execute_input":"2023-08-10T22:47:22.795820Z","iopub.status.idle":"2023-08-10T22:47:22.800334Z","shell.execute_reply.started":"2023-08-10T22:47:22.795784Z","shell.execute_reply":"2023-08-10T22:47:22.799270Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_length","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:22.801791Z","iopub.execute_input":"2023-08-10T22:47:22.802197Z","iopub.status.idle":"2023-08-10T22:47:22.813020Z","shell.execute_reply.started":"2023-08-10T22:47:22.802165Z","shell.execute_reply":"2023-08-10T22:47:22.812030Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"37"},"metadata":{}}]},{"cell_type":"code","source":"X_train = sequence.pad_sequences(train_sequences, maxlen=max_length)\nX_test = sequence.pad_sequences(test_sequences, maxlen=max_length)\nX_train.shape, X_test.shape","metadata":{"id":"pK3W-LRfbyk1","outputId":"1a8b9134-0110-4907-ea00-5fa89741aadf","execution":{"iopub.status.busy":"2023-08-10T22:47:22.814474Z","iopub.execute_input":"2023-08-10T22:47:22.814848Z","iopub.status.idle":"2023-08-10T22:47:22.884539Z","shell.execute_reply.started":"2023-08-10T22:47:22.814811Z","shell.execute_reply":"2023-08-10T22:47:22.883668Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((8052, 37), (2013, 37))"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 50\nEMBED_SIZE = 300\nLEARNING_RATE =  0.001\n\nearly_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0,\n    patience=10,\n    verbose=0,\n    mode='max',\n    baseline=None,\n    restore_best_weights=True)","metadata":{"id":"aNV13COwKUd5","execution":{"iopub.status.busy":"2023-08-10T22:47:22.887545Z","iopub.execute_input":"2023-08-10T22:47:22.889054Z","iopub.status.idle":"2023-08-10T22:47:22.894112Z","shell.execute_reply.started":"2023-08-10T22:47:22.889021Z","shell.execute_reply":"2023-08-10T22:47:22.893187Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def create_gru_model(model_type):\n    model = Sequential()\n    model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=max_length, trainable=True))\n    if model_type==1:\n        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(512, return_sequences=True)))\n        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(512, return_sequences=True)))\n        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=False)))\n        model.add(Dropout(0.5))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(64, activation='relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(8, activation='softmax'))\n    elif model_type==2:\n        model.add(tf.keras.layers.GRU(512, return_sequences=True))\n        model.add(tf.keras.layers.GRU(256, return_sequences=True))\n        model.add(tf.keras.layers.GRU(128, return_sequences=False))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(64, activation='relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(8, activation='softmax'))\n    elif model_type==3:\n        model.add(tf.keras.layers.GRU(128, return_sequences=False))\n        model.add(Dropout(0.5))\n        model.add(Flatten())\n        model.add(Dense(8, activation='softmax'))\n    elif model_type==4:\n        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=False)))\n        model.add(Dropout(0.5))\n        model.add(Flatten())\n        model.add(Dense(8, activation='softmax'))\n    elif model_type==5:\n        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=False)))\n        model.add(Dropout(0.5))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(8, activation='softmax'))\n    \n    opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    # model.summary()\n\n    history = model.fit(X_train, np.asarray(y_train), validation_data=(X_test, np.asarray(y_test)), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[early_stopping_monitor])\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:22.895404Z","iopub.execute_input":"2023-08-10T22:47:22.895882Z","iopub.status.idle":"2023-08-10T22:47:22.913877Z","shell.execute_reply.started":"2023-08-10T22:47:22.895850Z","shell.execute_reply":"2023-08-10T22:47:22.912999Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for i in range(1,6):\n    model, _ = create_gru_model(i)\n    predictions = model.predict(X_test)\n    print('--------------------------------------------------------')\n    print()\n    print('CLassification report for model {}: '.format(i))\n    print(classification_report(y_test.argmax(axis=1), predictions.argmax(axis=1), target_names=classes))\n    print()\n    print('--------------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T22:47:22.916116Z","iopub.execute_input":"2023-08-10T22:47:22.916437Z","iopub.status.idle":"2023-08-10T22:56:30.694210Z","shell.execute_reply.started":"2023-08-10T22:47:22.916413Z","shell.execute_reply":"2023-08-10T22:56:30.692649Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/50\n63/63 [==============================] - 32s 273ms/step - loss: 1.9356 - accuracy: 0.2393 - val_loss: 1.6314 - val_accuracy: 0.3895\nEpoch 2/50\n63/63 [==============================] - 12s 186ms/step - loss: 1.4393 - accuracy: 0.4839 - val_loss: 1.2854 - val_accuracy: 0.5445\nEpoch 3/50\n63/63 [==============================] - 11s 169ms/step - loss: 1.0278 - accuracy: 0.6338 - val_loss: 1.2852 - val_accuracy: 0.5718\nEpoch 4/50\n63/63 [==============================] - 8s 129ms/step - loss: 0.7144 - accuracy: 0.7541 - val_loss: 1.4133 - val_accuracy: 0.5509\nEpoch 5/50\n63/63 [==============================] - 8s 125ms/step - loss: 0.4773 - accuracy: 0.8536 - val_loss: 1.5855 - val_accuracy: 0.5763\nEpoch 6/50\n63/63 [==============================] - 7s 111ms/step - loss: 0.3104 - accuracy: 0.9128 - val_loss: 1.8936 - val_accuracy: 0.5743\nEpoch 7/50\n63/63 [==============================] - 6s 98ms/step - loss: 0.2126 - accuracy: 0.9477 - val_loss: 2.0917 - val_accuracy: 0.5743\nEpoch 8/50\n63/63 [==============================] - 6s 101ms/step - loss: 0.1556 - accuracy: 0.9646 - val_loss: 2.3762 - val_accuracy: 0.5758\nEpoch 9/50\n63/63 [==============================] - 6s 94ms/step - loss: 0.1009 - accuracy: 0.9780 - val_loss: 2.6297 - val_accuracy: 0.5847\nEpoch 10/50\n63/63 [==============================] - 6s 95ms/step - loss: 0.0961 - accuracy: 0.9788 - val_loss: 2.8390 - val_accuracy: 0.5708\nEpoch 11/50\n63/63 [==============================] - 6s 98ms/step - loss: 0.0905 - accuracy: 0.9809 - val_loss: 2.8365 - val_accuracy: 0.5807\nEpoch 12/50\n63/63 [==============================] - 5s 86ms/step - loss: 0.0755 - accuracy: 0.9821 - val_loss: 3.1982 - val_accuracy: 0.5772\nEpoch 13/50\n63/63 [==============================] - 6s 93ms/step - loss: 0.0918 - accuracy: 0.9809 - val_loss: 2.7333 - val_accuracy: 0.5733\nEpoch 14/50\n63/63 [==============================] - 5s 83ms/step - loss: 0.0645 - accuracy: 0.9865 - val_loss: 2.7881 - val_accuracy: 0.5782\nEpoch 15/50\n63/63 [==============================] - 5s 86ms/step - loss: 0.0481 - accuracy: 0.9888 - val_loss: 3.2508 - val_accuracy: 0.5673\nEpoch 16/50\n63/63 [==============================] - 6s 90ms/step - loss: 0.0582 - accuracy: 0.9871 - val_loss: 3.4055 - val_accuracy: 0.5638\nEpoch 17/50\n63/63 [==============================] - 5s 80ms/step - loss: 0.0763 - accuracy: 0.9831 - val_loss: 3.3329 - val_accuracy: 0.5812\nEpoch 18/50\n63/63 [==============================] - 6s 92ms/step - loss: 0.0474 - accuracy: 0.9909 - val_loss: 3.7606 - val_accuracy: 0.5708\nEpoch 19/50\n63/63 [==============================] - 5s 81ms/step - loss: 0.0529 - accuracy: 0.9902 - val_loss: 2.8101 - val_accuracy: 0.5559\n63/63 [==============================] - 3s 13ms/step\n--------------------------------------------------------\n\nCLassification report for model 1: \n              precision    recall  f1-score   support\n\n        none       0.62      0.67      0.64       307\n       anger       0.53      0.53      0.53       276\n         joy       0.50      0.41      0.45       268\n     sadness       0.37      0.38      0.37       258\n        love       0.66      0.61      0.63       250\n    sympathy       0.75      0.80      0.78       194\n    surprise       0.33      0.41      0.37       201\n        fear       0.96      0.88      0.92       259\n\n    accuracy                           0.58      2013\n   macro avg       0.59      0.59      0.59      2013\nweighted avg       0.59      0.58      0.59      2013\n\n\n--------------------------------------------------------\nEpoch 1/50\n63/63 [==============================] - 16s 162ms/step - loss: 1.7922 - accuracy: 0.3280 - val_loss: 1.3954 - val_accuracy: 0.4948\nEpoch 2/50\n63/63 [==============================] - 8s 125ms/step - loss: 1.1181 - accuracy: 0.6071 - val_loss: 1.2343 - val_accuracy: 0.5529\nEpoch 3/50\n63/63 [==============================] - 5s 83ms/step - loss: 0.6498 - accuracy: 0.7725 - val_loss: 1.3798 - val_accuracy: 0.5509\nEpoch 4/50\n63/63 [==============================] - 4s 55ms/step - loss: 0.3756 - accuracy: 0.8813 - val_loss: 1.7402 - val_accuracy: 0.5191\nEpoch 5/50\n63/63 [==============================] - 3s 49ms/step - loss: 0.2315 - accuracy: 0.9280 - val_loss: 1.8844 - val_accuracy: 0.5281\nEpoch 6/50\n63/63 [==============================] - 4s 68ms/step - loss: 0.1212 - accuracy: 0.9657 - val_loss: 2.3039 - val_accuracy: 0.5569\nEpoch 7/50\n63/63 [==============================] - 4s 61ms/step - loss: 0.0831 - accuracy: 0.9784 - val_loss: 2.5195 - val_accuracy: 0.5405\nEpoch 8/50\n63/63 [==============================] - 3s 51ms/step - loss: 0.0608 - accuracy: 0.9839 - val_loss: 2.5572 - val_accuracy: 0.5430\nEpoch 9/50\n63/63 [==============================] - 3s 48ms/step - loss: 0.0528 - accuracy: 0.9887 - val_loss: 2.6246 - val_accuracy: 0.5499\nEpoch 10/50\n63/63 [==============================] - 2s 36ms/step - loss: 0.0419 - accuracy: 0.9897 - val_loss: 2.8594 - val_accuracy: 0.5380\nEpoch 11/50\n63/63 [==============================] - 2s 35ms/step - loss: 0.0346 - accuracy: 0.9912 - val_loss: 2.7379 - val_accuracy: 0.5479\nEpoch 12/50\n63/63 [==============================] - 2s 30ms/step - loss: 0.0270 - accuracy: 0.9932 - val_loss: 2.9288 - val_accuracy: 0.5474\nEpoch 13/50\n63/63 [==============================] - 2s 30ms/step - loss: 0.0266 - accuracy: 0.9929 - val_loss: 3.1332 - val_accuracy: 0.5534\nEpoch 14/50\n63/63 [==============================] - 2s 27ms/step - loss: 0.0530 - accuracy: 0.9858 - val_loss: 2.9646 - val_accuracy: 0.5564\nEpoch 15/50\n63/63 [==============================] - 2s 36ms/step - loss: 0.0764 - accuracy: 0.9796 - val_loss: 2.5085 - val_accuracy: 0.5450\nEpoch 16/50\n63/63 [==============================] - 3s 43ms/step - loss: 0.0377 - accuracy: 0.9898 - val_loss: 2.8043 - val_accuracy: 0.5653\nEpoch 17/50\n63/63 [==============================] - 2s 28ms/step - loss: 0.0319 - accuracy: 0.9908 - val_loss: 2.9943 - val_accuracy: 0.5415\nEpoch 18/50\n63/63 [==============================] - 2s 25ms/step - loss: 0.0352 - accuracy: 0.9902 - val_loss: 2.9445 - val_accuracy: 0.5544\nEpoch 19/50\n63/63 [==============================] - 2s 32ms/step - loss: 0.0261 - accuracy: 0.9932 - val_loss: 2.8853 - val_accuracy: 0.5653\nEpoch 20/50\n63/63 [==============================] - 2s 37ms/step - loss: 0.0305 - accuracy: 0.9914 - val_loss: 2.9146 - val_accuracy: 0.5594\nEpoch 21/50\n63/63 [==============================] - 2s 25ms/step - loss: 0.0185 - accuracy: 0.9947 - val_loss: 3.0247 - val_accuracy: 0.5633\nEpoch 22/50\n63/63 [==============================] - 2s 31ms/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 3.0596 - val_accuracy: 0.5738\nEpoch 23/50\n63/63 [==============================] - 2s 29ms/step - loss: 0.0215 - accuracy: 0.9927 - val_loss: 3.1746 - val_accuracy: 0.5529\nEpoch 24/50\n63/63 [==============================] - 2s 31ms/step - loss: 0.0171 - accuracy: 0.9944 - val_loss: 3.5051 - val_accuracy: 0.5450\nEpoch 25/50\n63/63 [==============================] - 2s 40ms/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 3.1323 - val_accuracy: 0.5648\nEpoch 26/50\n63/63 [==============================] - 2s 32ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 3.3499 - val_accuracy: 0.5633\nEpoch 27/50\n63/63 [==============================] - 2s 25ms/step - loss: 0.0090 - accuracy: 0.9974 - val_loss: 3.5340 - val_accuracy: 0.5678\nEpoch 28/50\n63/63 [==============================] - 2s 25ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 3.5843 - val_accuracy: 0.5653\nEpoch 29/50\n63/63 [==============================] - 2s 38ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 3.7706 - val_accuracy: 0.5599\nEpoch 30/50\n63/63 [==============================] - 2s 38ms/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 3.7427 - val_accuracy: 0.5519\nEpoch 31/50\n63/63 [==============================] - 2s 33ms/step - loss: 0.0091 - accuracy: 0.9964 - val_loss: 3.7486 - val_accuracy: 0.5599\nEpoch 32/50\n63/63 [==============================] - 2s 34ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 3.8626 - val_accuracy: 0.5460\n63/63 [==============================] - 1s 5ms/step\n--------------------------------------------------------\n\nCLassification report for model 2: \n              precision    recall  f1-score   support\n\n        none       0.59      0.67      0.63       307\n       anger       0.55      0.47      0.51       276\n         joy       0.38      0.47      0.42       268\n     sadness       0.39      0.40      0.39       258\n        love       0.69      0.60      0.65       250\n    sympathy       0.76      0.73      0.74       194\n    surprise       0.38      0.37      0.37       201\n        fear       0.95      0.86      0.90       259\n\n    accuracy                           0.57      2013\n   macro avg       0.59      0.57      0.58      2013\nweighted avg       0.59      0.57      0.58      2013\n\n\n--------------------------------------------------------\nEpoch 1/50\n63/63 [==============================] - 13s 158ms/step - loss: 1.8546 - accuracy: 0.3269 - val_loss: 1.4606 - val_accuracy: 0.5012\nEpoch 2/50\n63/63 [==============================] - 7s 115ms/step - loss: 1.0749 - accuracy: 0.6484 - val_loss: 1.2009 - val_accuracy: 0.5897\nEpoch 3/50\n63/63 [==============================] - 4s 61ms/step - loss: 0.4549 - accuracy: 0.8680 - val_loss: 1.2941 - val_accuracy: 0.6006\nEpoch 4/50\n63/63 [==============================] - 3s 52ms/step - loss: 0.1618 - accuracy: 0.9578 - val_loss: 1.5458 - val_accuracy: 0.5797\nEpoch 5/50\n63/63 [==============================] - 2s 38ms/step - loss: 0.0758 - accuracy: 0.9820 - val_loss: 1.7032 - val_accuracy: 0.5946\nEpoch 6/50\n63/63 [==============================] - 2s 28ms/step - loss: 0.0439 - accuracy: 0.9904 - val_loss: 1.8017 - val_accuracy: 0.5922\nEpoch 7/50\n63/63 [==============================] - 1s 17ms/step - loss: 0.0368 - accuracy: 0.9914 - val_loss: 1.9846 - val_accuracy: 0.5902\nEpoch 8/50\n63/63 [==============================] - 2s 32ms/step - loss: 0.0289 - accuracy: 0.9943 - val_loss: 1.9945 - val_accuracy: 0.5892\nEpoch 9/50\n63/63 [==============================] - 2s 27ms/step - loss: 0.0215 - accuracy: 0.9962 - val_loss: 2.1344 - val_accuracy: 0.5897\nEpoch 10/50\n63/63 [==============================] - 2s 30ms/step - loss: 0.0195 - accuracy: 0.9957 - val_loss: 2.0426 - val_accuracy: 0.5897\nEpoch 11/50\n63/63 [==============================] - 1s 21ms/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 2.1073 - val_accuracy: 0.5922\nEpoch 12/50\n63/63 [==============================] - 1s 13ms/step - loss: 0.0143 - accuracy: 0.9962 - val_loss: 2.2189 - val_accuracy: 0.5827\nEpoch 13/50\n63/63 [==============================] - 1s 20ms/step - loss: 0.0133 - accuracy: 0.9968 - val_loss: 2.2237 - val_accuracy: 0.5787\n63/63 [==============================] - 0s 2ms/step\n--------------------------------------------------------\n\nCLassification report for model 3: \n              precision    recall  f1-score   support\n\n        none       0.59      0.72      0.65       307\n       anger       0.54      0.55      0.55       276\n         joy       0.53      0.39      0.45       268\n     sadness       0.41      0.37      0.39       258\n        love       0.63      0.69      0.66       250\n    sympathy       0.56      0.86      0.68       194\n    surprise       0.54      0.33      0.41       201\n        fear       0.96      0.90      0.93       259\n\n    accuracy                           0.60      2013\n   macro avg       0.60      0.60      0.59      2013\nweighted avg       0.60      0.60      0.59      2013\n\n\n--------------------------------------------------------\nEpoch 1/50\n63/63 [==============================] - 13s 142ms/step - loss: 1.8446 - accuracy: 0.3239 - val_loss: 1.4380 - val_accuracy: 0.4933\nEpoch 2/50\n63/63 [==============================] - 5s 78ms/step - loss: 1.0915 - accuracy: 0.6243 - val_loss: 1.1691 - val_accuracy: 0.5832\nEpoch 3/50\n63/63 [==============================] - 5s 82ms/step - loss: 0.5412 - accuracy: 0.8316 - val_loss: 1.1861 - val_accuracy: 0.6051\nEpoch 4/50\n63/63 [==============================] - 4s 66ms/step - loss: 0.2218 - accuracy: 0.9430 - val_loss: 1.2945 - val_accuracy: 0.6135\nEpoch 5/50\n63/63 [==============================] - 4s 57ms/step - loss: 0.0864 - accuracy: 0.9793 - val_loss: 1.5188 - val_accuracy: 0.6090\nEpoch 6/50\n63/63 [==============================] - 2s 38ms/step - loss: 0.0535 - accuracy: 0.9876 - val_loss: 1.5915 - val_accuracy: 0.6076\nEpoch 7/50\n63/63 [==============================] - 2s 33ms/step - loss: 0.0344 - accuracy: 0.9919 - val_loss: 1.7338 - val_accuracy: 0.6021\nEpoch 8/50\n63/63 [==============================] - 2s 26ms/step - loss: 0.0261 - accuracy: 0.9957 - val_loss: 1.8887 - val_accuracy: 0.5986\nEpoch 9/50\n63/63 [==============================] - 2s 35ms/step - loss: 0.0198 - accuracy: 0.9960 - val_loss: 1.9694 - val_accuracy: 0.6080\nEpoch 10/50\n63/63 [==============================] - 2s 38ms/step - loss: 0.0207 - accuracy: 0.9957 - val_loss: 1.9399 - val_accuracy: 0.6006\nEpoch 11/50\n63/63 [==============================] - 1s 20ms/step - loss: 0.0183 - accuracy: 0.9959 - val_loss: 1.9060 - val_accuracy: 0.6021\nEpoch 12/50\n63/63 [==============================] - 1s 20ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 2.0406 - val_accuracy: 0.5941\nEpoch 13/50\n63/63 [==============================] - 2s 33ms/step - loss: 0.0130 - accuracy: 0.9973 - val_loss: 2.1760 - val_accuracy: 0.6090\nEpoch 14/50\n63/63 [==============================] - 2s 36ms/step - loss: 0.0137 - accuracy: 0.9968 - val_loss: 2.1051 - val_accuracy: 0.6041\n63/63 [==============================] - 1s 3ms/step\n--------------------------------------------------------\n\nCLassification report for model 4: \n              precision    recall  f1-score   support\n\n        none       0.66      0.68      0.67       307\n       anger       0.53      0.67      0.59       276\n         joy       0.49      0.45      0.47       268\n     sadness       0.39      0.48      0.43       258\n        love       0.65      0.62      0.63       250\n    sympathy       0.82      0.70      0.75       194\n    surprise       0.57      0.39      0.46       201\n        fear       0.95      0.88      0.92       259\n\n    accuracy                           0.61      2013\n   macro avg       0.63      0.61      0.62      2013\nweighted avg       0.63      0.61      0.62      2013\n\n\n--------------------------------------------------------\nEpoch 1/50\n63/63 [==============================] - 14s 159ms/step - loss: 1.9170 - accuracy: 0.2835 - val_loss: 1.5874 - val_accuracy: 0.4709\nEpoch 2/50\n63/63 [==============================] - 6s 102ms/step - loss: 1.3085 - accuracy: 0.5455 - val_loss: 1.2360 - val_accuracy: 0.5733\nEpoch 3/50\n63/63 [==============================] - 4s 69ms/step - loss: 0.5929 - accuracy: 0.8068 - val_loss: 1.2434 - val_accuracy: 0.5946\nEpoch 4/50\n63/63 [==============================] - 4s 61ms/step - loss: 0.2172 - accuracy: 0.9355 - val_loss: 1.6966 - val_accuracy: 0.5713\nEpoch 5/50\n63/63 [==============================] - 3s 49ms/step - loss: 0.1022 - accuracy: 0.9706 - val_loss: 1.8201 - val_accuracy: 0.5772\nEpoch 6/50\n63/63 [==============================] - 3s 45ms/step - loss: 0.0702 - accuracy: 0.9812 - val_loss: 1.9761 - val_accuracy: 0.5837\nEpoch 7/50\n63/63 [==============================] - 2s 39ms/step - loss: 0.0492 - accuracy: 0.9880 - val_loss: 2.1932 - val_accuracy: 0.5857\nEpoch 8/50\n63/63 [==============================] - 2s 32ms/step - loss: 0.0365 - accuracy: 0.9918 - val_loss: 2.3965 - val_accuracy: 0.5807\nEpoch 9/50\n63/63 [==============================] - 2s 39ms/step - loss: 0.0300 - accuracy: 0.9927 - val_loss: 2.4500 - val_accuracy: 0.5897\nEpoch 10/50\n63/63 [==============================] - 2s 30ms/step - loss: 0.0267 - accuracy: 0.9938 - val_loss: 2.4509 - val_accuracy: 0.5768\nEpoch 11/50\n63/63 [==============================] - 2s 28ms/step - loss: 0.0245 - accuracy: 0.9944 - val_loss: 2.6001 - val_accuracy: 0.5748\nEpoch 12/50\n63/63 [==============================] - 2s 27ms/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 2.7607 - val_accuracy: 0.5782\nEpoch 13/50\n63/63 [==============================] - 2s 32ms/step - loss: 0.0247 - accuracy: 0.9940 - val_loss: 2.8172 - val_accuracy: 0.5688\n63/63 [==============================] - 1s 3ms/step\n--------------------------------------------------------\n\nCLassification report for model 5: \n              precision    recall  f1-score   support\n\n        none       0.59      0.82      0.69       307\n       anger       0.61      0.54      0.58       276\n         joy       0.40      0.53      0.46       268\n     sadness       0.40      0.34      0.37       258\n        love       0.70      0.57      0.63       250\n    sympathy       0.70      0.70      0.70       194\n    surprise       0.47      0.31      0.37       201\n        fear       0.92      0.88      0.90       259\n\n    accuracy                           0.59      2013\n   macro avg       0.60      0.58      0.59      2013\nweighted avg       0.60      0.59      0.59      2013\n\n\n--------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}