{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/yidinghao/interpreting-nlp#egg=interpret_nlp\n",
        "!pip install yattag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnqcD9M0EIG8",
        "outputId": "cfa6ed73-66de-407a-9dbe-6eacd214899a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting interpret_nlp\n",
            "  Cloning https://github.com/yidinghao/interpreting-nlp to /tmp/pip-install-mwmybrod/interpret-nlp_edec3213e28747bc8545451c20ac9700\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/yidinghao/interpreting-nlp /tmp/pip-install-mwmybrod/interpret-nlp_edec3213e28747bc8545451c20ac9700\n",
            "  Resolved https://github.com/yidinghao/interpreting-nlp to commit b6bf81e2e52970ffe4db32cade8c9c3d48d1cd3b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: interpret_nlp\n",
            "  Building wheel for interpret_nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for interpret_nlp: filename=interpret_nlp-0.1-py3-none-any.whl size=14664 sha256=3e9ddc6e0917b4f7689dc1a70cc6de4d6de1bb81e5b7cb2f6699da5db1fd291b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-szac3tcj/wheels/c0/d4/0f/9d21571e687d4e3dcf61b6538dedff103fdc48bad18ca6c357\n",
            "Successfully built interpret_nlp\n",
            "Installing collected packages: interpret_nlp\n",
            "Successfully installed interpret_nlp-0.1\n",
            "Requirement already satisfied: yattag in /usr/local/lib/python3.10/dist-packages (1.15.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c84CkUNzLgLO"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from interpret_nlp.modules.lrp_modules import LRPLinear, LRPGRU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWlmKYKULl2U",
        "outputId": "7924266d-6283-466f-b335-b915904314b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"/content/drive/MyDrive/Workspace/Copy of train.pkl\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/Workspace/Copy of test.pkl\"\n",
        "RS_TRAIN_PATH = \"/content/drive/MyDrive/Workspace/Copy of rs_train_embeddings.pkl\"\n",
        "RS_TEST_PATH = \"/content/drive/MyDrive/Workspace/Copy of rs_test_embeddings.pkl\""
      ],
      "metadata": {
        "id": "AuKhUtmvRI5C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(TRAIN_PATH, 'rb') as f:\n",
        "    train = pickle.load(f)\n",
        "\n",
        "with open(TEST_PATH, 'rb') as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "with open(RS_TRAIN_PATH, 'rb') as f:\n",
        "    rs_train_embeddings = pickle.load(f)\n",
        "\n",
        "with open(RS_TEST_PATH, 'rb') as f:\n",
        "    rs_test_embeddings = pickle.load(f)"
      ],
      "metadata": {
        "id": "SFQdDhAzQXdr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LRP Implementaion"
      ],
      "metadata": {
        "id": "2qL9ODCUfJCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "9qeoqzwiRm4r",
        "outputId": "9e9087a9-ec6c-4bde-d27f-71ae001c1ab4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  tweet  label  \\\n",
              "1426  شايف نفسه الحمار مع انه كان احتياط الموسم الما...      1   \n",
              "496   :   هزق البلد كلها ..مرتضي محدش عارف يوقفه ..ل...      1   \n",
              "3464  استفتاء : عندما تنتهي الاوليمبياد وتعود بعثتنا...      0   \n",
              "7073  Alhashemلا يوجد مخرج لنفط داعش سوي تركيا.هكذا ...      1   \n",
              "758   ما احب اركب جاهل معاي السياره عادي اتوتر طول م...      7   \n",
              "\n",
              "                                  tweet_with_out_emojis  \\\n",
              "1426  شايف نفسه الحمار مع انه كان احتياط الموسم الما...   \n",
              "496   :   هزق البلد كلها ..مرتضي محدش عارف يوقفه ..ل...   \n",
              "3464  استفتاء : عندما تنتهي الاوليمبياد وتعود بعثتنا...   \n",
              "7073  Alhashemلا يوجد مخرج لنفط داعش سوي تركيا.هكذا ...   \n",
              "758   ما احب اركب جاهل معاي السياره عادي اتوتر طول م...   \n",
              "\n",
              "                                           cleaned_data  \\\n",
              "1426  شايف الحمار انه احتياط الموسم الماضي اليوفي اد...   \n",
              "496   هزق البلد مرتضي محدش عارف يوقفه لا وزير داخليه...   \n",
              "3464  استفتاء تنتهي الاوليمبياد وتعود بعثتنا بالسلام...   \n",
              "7073  لا يوجد مخرج لنفط داعش سوي تركياهكذا تقول الجغ...   \n",
              "758   ما احب اركب جاهل معاي السياره عادي اتوتر طول م...   \n",
              "\n",
              "                                         Light Stemming  \\\n",
              "1426  شايف حمار انه احتياط موسم ماضي يوفي اداره ريال...   \n",
              "496   هزق بلد مرتضي محدش عارف يوقفه لا وزير داخليه ش...   \n",
              "3464  استفتاء تنتهي اوليمبياد وتعود بعثت سلامه ميدال...   \n",
              "7073  لا يوجد مخرج لنفط داعش سوي تركياهكذا تقول جغرا...   \n",
              "758   ما احب اركب جاهل معاي سياره عادي اتوتر طول ما ...   \n",
              "\n",
              "                                          Root Stemming  \n",
              "1426  شيف حمر انه حيط وسم اضي يوف دره ريل امر غرب هج...  \n",
              "496   هزق بلد رضي حدش عرف وقف لا وزر دخل شبب حتي بتع...  \n",
              "3464      فاء نهي اوليمبياد تعد بعث سلم يدل حصل وجه نظر  \n",
              "7073  لا وجد خرج نفط دعش سوي تركياهكذا تقل جغراف سؤل...  \n",
              "758   ما احب ركب جهل عاي سير عدي وتر طول ما انا اسق ...  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-e1f2cc87-5341-48da-aab7-ad1d197345d2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet_with_out_emojis</th>\n",
              "      <th>cleaned_data</th>\n",
              "      <th>Light Stemming</th>\n",
              "      <th>Root Stemming</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1426</th>\n",
              "      <td>شايف نفسه الحمار مع انه كان احتياط الموسم الما...</td>\n",
              "      <td>1</td>\n",
              "      <td>شايف نفسه الحمار مع انه كان احتياط الموسم الما...</td>\n",
              "      <td>شايف الحمار انه احتياط الموسم الماضي اليوفي اد...</td>\n",
              "      <td>شايف حمار انه احتياط موسم ماضي يوفي اداره ريال...</td>\n",
              "      <td>شيف حمر انه حيط وسم اضي يوف دره ريل امر غرب هج...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>:   هزق البلد كلها ..مرتضي محدش عارف يوقفه ..ل...</td>\n",
              "      <td>1</td>\n",
              "      <td>:   هزق البلد كلها ..مرتضي محدش عارف يوقفه ..ل...</td>\n",
              "      <td>هزق البلد مرتضي محدش عارف يوقفه لا وزير داخليه...</td>\n",
              "      <td>هزق بلد مرتضي محدش عارف يوقفه لا وزير داخليه ش...</td>\n",
              "      <td>هزق بلد رضي حدش عرف وقف لا وزر دخل شبب حتي بتع...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>استفتاء : عندما تنتهي الاوليمبياد وتعود بعثتنا...</td>\n",
              "      <td>0</td>\n",
              "      <td>استفتاء : عندما تنتهي الاوليمبياد وتعود بعثتنا...</td>\n",
              "      <td>استفتاء تنتهي الاوليمبياد وتعود بعثتنا بالسلام...</td>\n",
              "      <td>استفتاء تنتهي اوليمبياد وتعود بعثت سلامه ميدال...</td>\n",
              "      <td>فاء نهي اوليمبياد تعد بعث سلم يدل حصل وجه نظر</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7073</th>\n",
              "      <td>Alhashemلا يوجد مخرج لنفط داعش سوي تركيا.هكذا ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Alhashemلا يوجد مخرج لنفط داعش سوي تركيا.هكذا ...</td>\n",
              "      <td>لا يوجد مخرج لنفط داعش سوي تركياهكذا تقول الجغ...</td>\n",
              "      <td>لا يوجد مخرج لنفط داعش سوي تركياهكذا تقول جغرا...</td>\n",
              "      <td>لا وجد خرج نفط دعش سوي تركياهكذا تقل جغراف سؤل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>ما احب اركب جاهل معاي السياره عادي اتوتر طول م...</td>\n",
              "      <td>7</td>\n",
              "      <td>ما احب اركب جاهل معاي السياره عادي اتوتر طول م...</td>\n",
              "      <td>ما احب اركب جاهل معاي السياره عادي اتوتر طول م...</td>\n",
              "      <td>ما احب اركب جاهل معاي سياره عادي اتوتر طول ما ...</td>\n",
              "      <td>ما احب ركب جهل عاي سير عدي وتر طول ما انا اسق ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1f2cc87-5341-48da-aab7-ad1d197345d2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-4c08a150-8422-4212-b458-2640b0da81d3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c08a150-8422-4212-b458-2640b0da81d3')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-4c08a150-8422-4212-b458-2640b0da81d3 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1f2cc87-5341-48da-aab7-ad1d197345d2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1f2cc87-5341-48da-aab7-ad1d197345d2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(rs_train_embeddings[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIJwvMFCRvLD",
        "outputId": "1b269fee-c7d7-4bed-d320-3fa6532248d1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()\n",
        "\n",
        "y_train = encoder.fit_transform(train['label'].values.reshape(-1,1)).toarray()\n",
        "y_test = encoder.transform(test['label'].values.reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "RL9a8X96UjEo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_gru_global = rs_train_embeddings.reshape(rs_train_embeddings.shape[0], 1, rs_train_embeddings.shape[1])\n",
        "X_test_gru_global = rs_test_embeddings.reshape(rs_test_embeddings.shape[0], 1, rs_test_embeddings.shape[1])"
      ],
      "metadata": {
        "id": "JG5WQoQaTpSh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handle out of vocab words\n",
        "tokenizer = Tokenizer(oov_token='')\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train[\"Root Stemming\"])\n",
        "tokenizer.word_index[''] = 0"
      ],
      "metadata": {
        "id": "TKCmkd68xKyQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the words with the max index\n",
        "word_with_max_idx = max([(k, v) for k, v in tokenizer.word_index.items()], key = lambda x:x[1])\n",
        "# showing the words with the min index\n",
        "word_with_min_idx = min([(k, v) for k, v in tokenizer.word_index.items()], key = lambda x:x[1])\n",
        "idx_of_UNK = tokenizer.word_index['']\n",
        "print(word_with_max_idx, word_with_min_idx, idx_of_UNK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f97ff10-e1b3-4fa4-a79b-97e8daddbcbc",
        "id": "O629_BHJxKyS"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('هتد', 8644) ('', 0) 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train[\"Root Stemming\"])\n",
        "test_sequences = tokenizer.texts_to_sequences(test[\"Root Stemming\"])"
      ],
      "metadata": {
        "id": "4O_ZWq2fxKyT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myDataset(Dataset):\n",
        "  def __init__(self, train_sequences, labels):\n",
        "      # save data\n",
        "      self.train_sequences = train_sequences\n",
        "      self.labels = labels\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      # retrieve data\n",
        "      train_sequence = self.train_sequences[index]\n",
        "      label = self.labels[index]\n",
        "\n",
        "      return (torch.tensor(train_sequence), torch.tensor(label))\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.labels)"
      ],
      "metadata": {
        "id": "2h22Y9ZQQ0Me"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTMEmotionModel(nn.Module):\n",
        "#   def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n",
        "#     super(LSTMEmotionModel, self).__init__()\n",
        "#     self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "#     self.hidden_size = hidden_size\n",
        "#     self.num_layers = num_layers\n",
        "#     self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "#     self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     embedded = self.embedding(x)\n",
        "#     h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "#     c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "#     out, _ = self.lstm(embedded, (h0, c0))\n",
        "#     out = self.fc(out[:, -1, :])  # Take the last time step's output\n",
        "#     return out"
      ],
      "metadata": {
        "id": "t6kifeMSNnGF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
        "    super(GRUClassifier, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # initial_embeddings = torch.tensor(rs_train_embeddings)\n",
        "    # self.embedding = nn.Embedding.from_pretrained(initial_embeddings, freeze=False)\n",
        "    self.gru = LRPGRU(embedding_dim, hidden_size, bidirectional=True)\n",
        "    self.linear = LRPLinear(hidden_size, 8)\n",
        "\n",
        "  def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
        "    embeddings = self.embedding(x)\n",
        "    hidden = self.gru(embeddings)[1][0]\n",
        "    hidden = hidden.view(-1, hidden_size)\n",
        "    out = self.linear(hidden)\n",
        "    return out"
      ],
      "metadata": {
        "id": "A-FKHka6EDLL"
      },
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaSN_fcws14M",
        "outputId": "22c0b82e-36b0-4316-ac1b-14e7e3f318dd"
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8052, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 50\n",
        "x_train = sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(test_sequences, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce65e4a6-c53a-44bd-f0f9-725c326cedaa",
        "id": "YJBB0fHqxKyU"
      },
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (8052, 50)\n",
            "x_test shape: (2013, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customDataset = myDataset(x_train, y_train)\n",
        "# split the list into training and testing sets\n",
        "train_data, val_data = train_test_split(customDataset, train_size=0.95)"
      ],
      "metadata": {
        "id": "uhlPejwJR1_h"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "# creating training and validation data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(val_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "DVn2Iv7UShkA"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 8\n",
        "# vocab_size = len(rs_train_embeddings)  # Size of your vocabulary\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = rs_train_embeddings.shape[1]  # Size of your embedding vectors"
      ],
      "metadata": {
        "id": "Fw6J11VFNgzu"
      },
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_train_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjR_IsIMO0bw",
        "outputId": "73effd18-93af-4e7a-cc6d-c1e23f2a0cfa"
      },
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8052, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 368
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize the model\n",
        "# model = LSTMEmotionModel(vocab_size, embedding_dim, hidden_size, num_layers, num_classes)"
      ],
      "metadata": {
        "id": "9YnMUTJJN-bj"
      },
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = GRUClassifier(vocab_size, embedding_dim, hidden_size, num_layers)"
      ],
      "metadata": {
        "id": "Iua8lJ4BFMC_"
      },
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ifwViDJtOlPq"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    labels = torch.argmax(labels, dim=1)  # Convert one-hot labels to integer labels\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training progress\n",
        "    print(f'Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cft6RcteOnsa",
        "outputId": "494decf1-25eb-4373-f3b9-2d55da606f81"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.1170\n",
            "Loss: 2.0755\n",
            "Loss: 2.0509\n",
            "Loss: 2.0053\n",
            "Loss: 1.9920\n",
            "Loss: 2.0003\n",
            "Loss: 1.9581\n",
            "Loss: 1.9372\n",
            "Loss: 1.9445\n",
            "Loss: 1.9929\n",
            "Loss: 1.9428\n",
            "Loss: 1.9497\n",
            "Loss: 1.8569\n",
            "Loss: 1.8537\n",
            "Loss: 1.8360\n",
            "Loss: 1.8101\n",
            "Loss: 1.7981\n",
            "Loss: 1.8407\n",
            "Loss: 1.7920\n",
            "Loss: 1.8250\n",
            "Loss: 1.8574\n",
            "Loss: 1.8671\n",
            "Loss: 1.8509\n",
            "Loss: 1.8265\n",
            "Loss: 1.8409\n",
            "Loss: 1.7545\n",
            "Loss: 1.7117\n",
            "Loss: 1.8158\n",
            "Loss: 1.7231\n",
            "Loss: 1.6849\n",
            "Loss: 1.6498\n",
            "Loss: 1.6456\n",
            "Loss: 1.7342\n",
            "Loss: 1.6331\n",
            "Loss: 1.6849\n",
            "Loss: 1.6731\n",
            "Loss: 1.6588\n",
            "Loss: 1.6819\n",
            "Loss: 1.6618\n",
            "Loss: 1.6060\n",
            "Loss: 1.6170\n",
            "Loss: 1.5725\n",
            "Loss: 1.5879\n",
            "Loss: 1.5881\n",
            "Loss: 1.5800\n",
            "Loss: 1.5820\n",
            "Loss: 1.6190\n",
            "Loss: 1.4612\n",
            "Loss: 1.6709\n",
            "Loss: 1.5756\n",
            "Loss: 1.4351\n",
            "Loss: 1.4434\n",
            "Loss: 1.4663\n",
            "Loss: 1.6832\n",
            "Loss: 1.5868\n",
            "Loss: 1.4544\n",
            "Loss: 1.5016\n",
            "Loss: 1.5745\n",
            "Loss: 1.5696\n",
            "Loss: 1.5380\n",
            "Loss: 1.3471\n",
            "Loss: 1.2737\n",
            "Loss: 1.1685\n",
            "Loss: 1.1580\n",
            "Loss: 1.2597\n",
            "Loss: 1.1675\n",
            "Loss: 1.1644\n",
            "Loss: 1.1625\n",
            "Loss: 1.2283\n",
            "Loss: 1.1884\n",
            "Loss: 1.1483\n",
            "Loss: 1.1357\n",
            "Loss: 1.1393\n",
            "Loss: 1.2567\n",
            "Loss: 1.2524\n",
            "Loss: 1.1071\n",
            "Loss: 1.0125\n",
            "Loss: 1.0691\n",
            "Loss: 1.1253\n",
            "Loss: 1.1207\n",
            "Loss: 1.0569\n",
            "Loss: 1.1775\n",
            "Loss: 1.0249\n",
            "Loss: 1.0439\n",
            "Loss: 1.0634\n",
            "Loss: 1.1553\n",
            "Loss: 0.9792\n",
            "Loss: 1.1299\n",
            "Loss: 1.1608\n",
            "Loss: 1.2710\n",
            "Loss: 1.1355\n",
            "Loss: 1.0889\n",
            "Loss: 1.0790\n",
            "Loss: 0.9870\n",
            "Loss: 0.8618\n",
            "Loss: 0.9482\n",
            "Loss: 0.9910\n",
            "Loss: 1.0023\n",
            "Loss: 1.1220\n",
            "Loss: 0.9807\n",
            "Loss: 1.1477\n",
            "Loss: 1.0408\n",
            "Loss: 0.9550\n",
            "Loss: 0.9747\n",
            "Loss: 0.9050\n",
            "Loss: 0.9726\n",
            "Loss: 1.0764\n",
            "Loss: 0.9044\n",
            "Loss: 1.1203\n",
            "Loss: 0.9827\n",
            "Loss: 0.9561\n",
            "Loss: 1.0648\n",
            "Loss: 1.0484\n",
            "Loss: 0.9704\n",
            "Loss: 1.0397\n",
            "Loss: 0.9333\n",
            "Loss: 1.0584\n",
            "Loss: 1.0679\n",
            "Loss: 0.8646\n",
            "Loss: 1.0635\n",
            "Loss: 0.7623\n",
            "Loss: 0.6808\n",
            "Loss: 0.5631\n",
            "Loss: 0.7695\n",
            "Loss: 0.7487\n",
            "Loss: 0.7322\n",
            "Loss: 0.7030\n",
            "Loss: 0.6471\n",
            "Loss: 0.6752\n",
            "Loss: 0.7435\n",
            "Loss: 0.6808\n",
            "Loss: 0.6766\n",
            "Loss: 0.6179\n",
            "Loss: 0.7869\n",
            "Loss: 0.5925\n",
            "Loss: 0.7489\n",
            "Loss: 0.7093\n",
            "Loss: 0.6596\n",
            "Loss: 0.7222\n",
            "Loss: 0.6299\n",
            "Loss: 0.6534\n",
            "Loss: 0.6723\n",
            "Loss: 0.6769\n",
            "Loss: 0.6691\n",
            "Loss: 0.6752\n",
            "Loss: 0.7706\n",
            "Loss: 0.6479\n",
            "Loss: 0.7071\n",
            "Loss: 0.6417\n",
            "Loss: 0.6346\n",
            "Loss: 0.6602\n",
            "Loss: 0.7326\n",
            "Loss: 0.6725\n",
            "Loss: 0.6341\n",
            "Loss: 0.7487\n",
            "Loss: 0.7182\n",
            "Loss: 0.7069\n",
            "Loss: 0.5866\n",
            "Loss: 0.6648\n",
            "Loss: 0.6235\n",
            "Loss: 0.7244\n",
            "Loss: 0.6285\n",
            "Loss: 0.7729\n",
            "Loss: 0.7300\n",
            "Loss: 0.6491\n",
            "Loss: 0.7195\n",
            "Loss: 0.6166\n",
            "Loss: 0.6430\n",
            "Loss: 0.6482\n",
            "Loss: 0.6447\n",
            "Loss: 0.7596\n",
            "Loss: 0.6346\n",
            "Loss: 0.5219\n",
            "Loss: 0.5593\n",
            "Loss: 0.5178\n",
            "Loss: 0.6154\n",
            "Loss: 0.5031\n",
            "Loss: 0.6541\n",
            "Loss: 0.7465\n",
            "Loss: 0.7583\n",
            "Loss: 0.4143\n",
            "Loss: 0.3333\n",
            "Loss: 0.3818\n",
            "Loss: 0.4289\n",
            "Loss: 0.4329\n",
            "Loss: 0.3314\n",
            "Loss: 0.3757\n",
            "Loss: 0.3424\n",
            "Loss: 0.3486\n",
            "Loss: 0.3748\n",
            "Loss: 0.3628\n",
            "Loss: 0.3604\n",
            "Loss: 0.3815\n",
            "Loss: 0.3394\n",
            "Loss: 0.3698\n",
            "Loss: 0.3987\n",
            "Loss: 0.3584\n",
            "Loss: 0.3838\n",
            "Loss: 0.4067\n",
            "Loss: 0.3206\n",
            "Loss: 0.3438\n",
            "Loss: 0.4063\n",
            "Loss: 0.3702\n",
            "Loss: 0.2917\n",
            "Loss: 0.3539\n",
            "Loss: 0.3676\n",
            "Loss: 0.3324\n",
            "Loss: 0.3302\n",
            "Loss: 0.3249\n",
            "Loss: 0.3901\n",
            "Loss: 0.3225\n",
            "Loss: 0.4167\n",
            "Loss: 0.4017\n",
            "Loss: 0.3216\n",
            "Loss: 0.3087\n",
            "Loss: 0.3123\n",
            "Loss: 0.3218\n",
            "Loss: 0.3928\n",
            "Loss: 0.3678\n",
            "Loss: 0.3967\n",
            "Loss: 0.3301\n",
            "Loss: 0.4006\n",
            "Loss: 0.3466\n",
            "Loss: 0.4018\n",
            "Loss: 0.3487\n",
            "Loss: 0.4595\n",
            "Loss: 0.3454\n",
            "Loss: 0.3144\n",
            "Loss: 0.4442\n",
            "Loss: 0.3376\n",
            "Loss: 0.4308\n",
            "Loss: 0.4039\n",
            "Loss: 0.4080\n",
            "Loss: 0.4071\n",
            "Loss: 0.4841\n",
            "Loss: 0.4069\n",
            "Loss: 0.3708\n",
            "Loss: 0.3645\n",
            "Loss: 0.3349\n",
            "Loss: 0.2573\n",
            "Loss: 0.1849\n",
            "Loss: 0.2060\n",
            "Loss: 0.1634\n",
            "Loss: 0.1916\n",
            "Loss: 0.2437\n",
            "Loss: 0.1651\n",
            "Loss: 0.1898\n",
            "Loss: 0.1660\n",
            "Loss: 0.1852\n",
            "Loss: 0.1640\n",
            "Loss: 0.1824\n",
            "Loss: 0.2031\n",
            "Loss: 0.2009\n",
            "Loss: 0.1468\n",
            "Loss: 0.1502\n",
            "Loss: 0.2322\n",
            "Loss: 0.1875\n",
            "Loss: 0.1414\n",
            "Loss: 0.1271\n",
            "Loss: 0.1669\n",
            "Loss: 0.1835\n",
            "Loss: 0.2512\n",
            "Loss: 0.1664\n",
            "Loss: 0.1856\n",
            "Loss: 0.1694\n",
            "Loss: 0.1543\n",
            "Loss: 0.1301\n",
            "Loss: 0.1542\n",
            "Loss: 0.2316\n",
            "Loss: 0.1929\n",
            "Loss: 0.1760\n",
            "Loss: 0.1947\n",
            "Loss: 0.2048\n",
            "Loss: 0.2302\n",
            "Loss: 0.1864\n",
            "Loss: 0.1730\n",
            "Loss: 0.1952\n",
            "Loss: 0.1715\n",
            "Loss: 0.1535\n",
            "Loss: 0.1934\n",
            "Loss: 0.1634\n",
            "Loss: 0.1542\n",
            "Loss: 0.1583\n",
            "Loss: 0.1616\n",
            "Loss: 0.1420\n",
            "Loss: 0.1923\n",
            "Loss: 0.1745\n",
            "Loss: 0.1194\n",
            "Loss: 0.1691\n",
            "Loss: 0.1740\n",
            "Loss: 0.2256\n",
            "Loss: 0.2272\n",
            "Loss: 0.2022\n",
            "Loss: 0.1692\n",
            "Loss: 0.2058\n",
            "Loss: 0.2050\n",
            "Loss: 0.1751\n",
            "Loss: 0.2691\n",
            "Loss: 0.2490\n",
            "Loss: 0.1783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for inputs, labels in test_loader:\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      labels = torch.argmax(labels, dim=1)  # Convert one-hot labels to integer labels\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Test Accuracy: {:.2f}%'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URq9m22DMMGF",
        "outputId": "2011b930-ec80-40ec-8451-405b627f2577"
      },
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 62.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model state dictionary\n",
        "torch.save(model.state_dict(), 'model_state.pth')"
      ],
      "metadata": {
        "id": "nRPz4JnR6tTm"
      },
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize the model architecture\n",
        "# model = LSTMEmotionModel(vocab_size, embedding_dim, hidden_size, num_layers, num_classes)\n",
        "\n",
        "# # Load the saved model state dictionary\n",
        "model.load_state_dict(torch.load('model_state.pth'))\n",
        "model.eval()  # Set the model to evaluation mode"
      ],
      "metadata": {
        "id": "y2LTTswQ9Fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af56a1a5-7fa4-4848-af21-f05da4649420"
      },
      "execution_count": 433,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRUClassifier(\n",
              "  (embedding): Embedding(8645, 768)\n",
              "  (gru): LRPGRU(768, 128, batch_first=True, bidirectional=True)\n",
              "  (linear): LRPLinear(in_features=128, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 433
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the entire model\n",
        "# torch.save(model, 'complete_model.pth')"
      ],
      "metadata": {
        "id": "9YkxlB459TPc"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the entire model\n",
        "# loaded_model = torch.load('complete_model.pth')\n",
        "# loaded_model.eval()  # Set the loaded model to evaluation mode"
      ],
      "metadata": {
        "id": "qV3mmskH_RGC"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(text):\n",
        "  tokinized = tokenizer.texts_to_sequences(text)\n",
        "  seq = sequence.pad_sequences(tokinized, maxlen=maxlen)\n",
        "  print(seq.shape)\n",
        "  return torch.LongTensor(seq)"
      ],
      "metadata": {
        "id": "uPngR1CvQKkq"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0].reshape(1,-1)"
      ],
      "metadata": {
        "id": "A7m2R0zzUD1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3bcdd5-c293-4bd5-8d6b-d2d156336d7e"
      },
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0, 626, 242, 994,   7,  38, 767]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 437
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.LongTensor(x_test[0].reshape(1,-1))\n",
        "print(x.shape)\n",
        "y = model(x).detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzHq4Bq3P1BX",
        "outputId": "37d4d098-0151-4e78-921f-2e08fd25f0f4"
      },
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avwiOFcBsQAD",
        "outputId": "44ae0616-2033-407a-8aa5-db1c0bc4b64c"
      },
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 439
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LRP forward\n",
        "model.linear.attr()\n",
        "model.gru.attr()"
      ],
      "metadata": {
        "id": "3xACJ9_pST9g"
      },
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.embedding(x)\n",
        "gru_out = model.gru(embeddings.detach().numpy())  # 1, 8, 120"
      ],
      "metadata": {
        "id": "i2UtJimVarQP"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8uZRqdzasDH",
        "outputId": "7b128ce4-5aaf-4100-faf1-0272d9724b55"
      },
      "execution_count": 442,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 50, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 442
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h_final = np.concatenate((gru_out[:, -1, :128], gru_out[:, 0, 128:]), axis=-1)\n",
        "h_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPDEoyVpbGUU",
        "outputId": "009a7092-fa51-4c61-8576-9541cf05a274"
      },
      "execution_count": 443,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 443
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output = model.linear(h_final)\n",
        "h_final_reshaped = h_final.reshape(-1, model.linear.weight.shape[1])\n",
        "# Calculate the output\n",
        "output = model.linear(torch.tensor(h_final_reshaped, dtype=torch.float32))"
      ],
      "metadata": {
        "id": "yldTu9BCSaJV"
      },
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKHY1CndbQGp",
        "outputId": "0e448497-0228-452a-c96e-c226dec26296"
      },
      "execution_count": 445,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 445
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LRP backward\n",
        "target_class = np.argmax(output, 1)\n",
        "rel_output = np.zeros(output.shape)\n",
        "rel_output[:, target_class] = output[:, target_class]\n",
        "rel_h_final = model.linear.attr_backward(rel_output)"
      ],
      "metadata": {
        "id": "5xUm2c7PTzH3"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_gru_out = np.zeros(gru_out.shape)"
      ],
      "metadata": {
        "id": "gELOQHFnbiwK"
      },
      "execution_count": 447,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_gru_out.shape, rel_h_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9XUn9qDbkg_",
        "outputId": "e84bc385-58a0-4185-f7bf-fe72d901ef92"
      },
      "execution_count": 448,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 50, 256), (2, 128))"
            ]
          },
          "metadata": {},
          "execution_count": 448
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel_gru_out[:, -1, :128].shape, rel_h_final[:, :].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcLrViUhbuEW",
        "outputId": "4a9fd05b-1cca-42d1-a700-26165fdab4f9"
      },
      "execution_count": 449,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 128), (2, 128))"
            ]
          },
          "metadata": {},
          "execution_count": 449
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel_gru_out[:, 0, 128:].shape, rel_h_final[:, :].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB-bEUFAcFq0",
        "outputId": "e65017ab-a047-4432-c048-f96b621ac7b0"
      },
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 128), (2, 128))"
            ]
          },
          "metadata": {},
          "execution_count": 450
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel_gru_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89CeM0jnyLA0",
        "outputId": "bbfb316b-9541-4aea-fbf3-8f87222679f4"
      },
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 50, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 451
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel_h_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ZwBGRzyNaD",
        "outputId": "ed403e70-35c9-4f9d-e8f2-5da1a234323e"
      },
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 452
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rel_gru_out[:, -1, :60] = rel_h_final[:, :60]\n",
        "# rel_gru_out[:, 0, 60:] = rel_h_final[:, 60:]\n",
        "rel_gru_out[:, -1, :128] = rel_h_final[:, :64].reshape((1,128))\n",
        "rel_gru_out[:, 0, 128:] = rel_h_final[:, 64:].reshape((1,128))"
      ],
      "metadata": {
        "id": "9zHWoMexc7ue"
      },
      "execution_count": 453,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_gru_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsS4tvxPdES5",
        "outputId": "99d16c2d-cfee-4d0c-c164-e17d61cd4923"
      },
      "execution_count": 454,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 50, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 454
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rel_gru_out.reshape((25, 50, 256))"
      ],
      "metadata": {
        "id": "wlGa5y_cecAW"
      },
      "execution_count": 455,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_embeddings = model.gru.attr_backward(rel_gru_out)"
      ],
      "metadata": {
        "id": "VfecIpJCc-37"
      },
      "execution_count": 457,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8NjXuzIz9wn",
        "outputId": "c4416efd-8cd7-489d-ad8f-96cbf9bc5629"
      },
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 50, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 462
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Relevance Scores:\")\n",
        "for i, rel_w in enumerate(rel_embeddings.sum(axis=-1).squeeze()):\n",
        "    print(\"    {}: {:.4f}\".format(x_test[0][i], rel_w))"
      ],
      "metadata": {
        "id": "kcVbFE6h_Wyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf209c04-6ffa-476f-9e10-5abb04efcb73"
      },
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevance Scores:\n",
            "    0: 15498.7839\n",
            "    0: 1261.9948\n",
            "    0: 139.4122\n",
            "    0: 1.8828\n",
            "    0: -20.3136\n",
            "    0: -17.0793\n",
            "    0: 10.9958\n",
            "    0: 134.4200\n",
            "    0: 721.3337\n",
            "    0: 3720.2684\n",
            "    0: 20111.8979\n",
            "    0: 116299.7968\n",
            "    0: 726128.7392\n",
            "    0: 4936352.8957\n",
            "    0: 36893369.0947\n",
            "    0: 306735197.7527\n",
            "    0: 2879440597.8186\n",
            "    0: 31115412175.1212\n",
            "    0: 397305058627.3771\n",
            "    0: 6222788654892.7207\n",
            "    0: 126668723281428.0938\n",
            "    0: 3709299365651620.0000\n",
            "    0: 196180105358115360.0000\n",
            "    0: 57776215301763399680.0000\n",
            "    0: -3974700579458029453312.0000\n",
            "    0: 129836084008085471887360.0000\n",
            "    0: -2763487398178540637650944.0000\n",
            "    0: 43410916183202776899125248.0000\n",
            "    0: -538390243056092696381751296.0000\n",
            "    0: 5496392450383243154745720832.0000\n",
            "    0: -47540299564969617153706688512.0000\n",
            "    0: 355796368926826496715145609216.0000\n",
            "    0: -2341329381828245561735039156224.0000\n",
            "    0: 13713908501897484883422431150080.0000\n",
            "    0: -72207880782022820902143868796928.0000\n",
            "    0: 344627748410059877365668141596672.0000\n",
            "    0: -1501447547289900417079351391027200.0000\n",
            "    0: 6005789472124141420340605985226752.0000\n",
            "    0: -22160575324863412485049572875304960.0000\n",
            "    0: 75774019976503567933119961753452544.0000\n",
            "    0: -240981873224163354686810962637357056.0000\n",
            "    0: 722777797866934077986205181805592576.0000\n",
            "    0: -2057607869092578663554758642038210560.0000\n",
            "    0: 7825217424704966707291707707957968896.0000\n",
            "    626: 48087486379838445538841292809086959616.0000\n",
            "    242: 18319986125276882903369666616184799232.0000\n",
            "    994: -152663559429843057891836535041425408.0000\n",
            "    7: -241509365452073323515433859259230584832.0000\n",
            "    38: 175752743980631584114209564218502414336.0000\n",
            "    767: 263430221999762327101801591168762380288.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rnH3Ab7XxYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}